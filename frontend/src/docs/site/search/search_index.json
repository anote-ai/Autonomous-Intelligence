{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"10-ks/10ksbackground.html","title":"10-Ks Background","text":""},{"location":"10-ks/10ksbackground.html#extracting-information-from-10-ks","title":"Extracting Information from 10-Ks","text":"<p>10-Ks, or annual reports filed by publicly-traded companies with the United States Securities and Exchange Commission (SEC), provide a comprehensive analysis of a company's performance. These critical documents often span hundreds of pages and include sections like the business overview, risk factors, financial statements, and market-related data. Unearthing information from 10-Ks can give investors and stakeholders a deep understanding of the company's overall health and future prospects. Here are some specific use cases:</p>"},{"location":"10-ks/10ksbackground.html#interest-rates","title":"Interest rates","text":"<p>The financial section of a 10-K often includes information on a company's interest rate exposure, such as details about interest rate swaps, floating rates on debt, and hedging strategies. Extracting this data accurately and promptly can give analysts a clear picture of the company's interest rate risk. This crucial information could be used to tailor investment strategies, hedge interest rate risk, and make risk management decisions, thus helping to avoid potential financial pitfalls and optimize returns.</p>"},{"location":"10-ks/10ksbackground.html#know-your-customer-kyc","title":"Know Your Customer (KYC)","text":"<p>In the context of corporate analysis, KYC could involve extracting data about the company's location, industry sector, board of directors, total assets, and ownership structure. Understanding these attributes aids in due diligence, as well as helps analysts build a comprehensive profile of their customer - in this case, the company. It also aids in maintaining regulatory compliance and helps financial institutions to avoid doing business with entities involved in fraudulent activities.</p>"},{"location":"10-ks/10ksbackground.html#risk-factors","title":"Risk factors","text":"<p>Every 10-K includes a section that outlines potential risks and uncertainties that the company faces. These risks could range from operational and strategic risks to industry-specific and macroeconomic risks. Extracting and analyzing these risk factors from 10-Ks allow analysts and investors to assess the overall risk landscape of the company. They can then factor this into their investment decisions, risk management plans, and strategic decision-making, potentially avoiding businesses with high risk and spotting investment opportunities in companies with manageable risk profiles.</p>"},{"location":"10-ks/10ksbackground.html#challenges","title":"Challenges","text":"<p>Extracting information from 10-Ks is very important. However, the process of extracting this valuable information from 10-Ks poses several challenges that warrant careful consideration.</p>"},{"location":"10-ks/10ksbackground.html#length-and-effort","title":"Length and Effort","text":"<p>10-K reports, ranging from 100 to 350 pages, demand significant effort to extract key insights. Manually reviewing these extensive documents can be an arduous and time-consuming task, requiring meticulous attention to detail.</p>"},{"location":"10-ks/10ksbackground.html#manual-intervention-and-resource-intensiveness","title":"Manual Intervention and Resource Intensiveness","text":"<p>Extracting the desired information from 10-Ks often involves extensive manual intervention from analysts and subject-matter experts (SMEs). Analysts meticulously review each document, identifying relevant data points, and manually labeling them. Unfortunately, this approach proves to be time-consuming, costly, and inefficient. For example, extracting information about interest rate swaps may necessitate a team of 45 analysts dedicating 15 hours per week, summing up to a staggering 260 hours for a single project. Similarly, KYC analysis demands a significant investment of time and resources, with companies employing up to 500 analysts spending around 70 minutes per 10-K document.</p>"},{"location":"10-ks/10ksbackground.html#variability-in-data-representation","title":"Variability in Data Representation","text":"<p>Another significant challenge stems from the diverse formats in which data is represented within 10-Ks. These reports consist of a mix of structured and unstructured data, including tables, paragraphs, and semi-structured information. Effectively extracting relevant information from such diverse formats demands sophisticated techniques and a deep understanding of the domain. Moreover, the lack of a standardized layout and format complicates the application of machine learning models, which must adapt to various document structures and data representations.</p>"},{"location":"10-ks/10ksbackground.html#volume-of-information","title":"Volume of Information","text":"<p>Another main challenges arises from the sheer volume of 10-K documents to analyze. As public companies are required to file 10-K reports annually, the collection of documents for a single company could span multiple years, resulting in a substantial amount of data to review. Additionally, each 10-K document contained complex financial statements, legal disclosures, and management discussions, making it a time-consuming and labor-intensive task to extract meaningful information manually.</p>"},{"location":"10-ks/10ksdecomposition.html","title":"10-Ks Decomposition","text":""},{"location":"10-ks/10ksdecomposition.html#example-of-decomposition-of-10-ks","title":"Example of Decomposition of 10-Ks","text":"<p>Decomposing 10-K reports involves extracting sections such as the business overview, financial statements, risk factors, and management discussions. By decomposing 10-Ks, we can analyze the specific sections and extract relevant information for financial analysis, regulatory compliance, or investment decision-making. Here is a code example of how we use unstructured.io to perform document specific decomposition of 10-Ks:</p> <p>With robust document decomposition, we can find key information in 10-Ks, as seen in the image below:</p> <p> </p>"},{"location":"10-ks/10ksfeedback.html","title":"10-Ks Feedback","text":""},{"location":"10-ks/10ksfeedback.html#prompt-inventory-and-human-adjustment","title":"Prompt Inventory and Human Adjustment","text":"<p>When dealing with domain-specific documents, such as financial statements, the language model may sometimes provide incorrect or inaccurate answers for specific questions. For instance, when you ask about the major drivers of revenue and profit growth or decline, you may find the model's response to be too general, lacking specific information from the company's financial statements.</p> <p> </p> <p>To address this issue and improve the accuracy of responses, we offer user-specific prompt inventories. This means that you can create and store all the relevant questions related to financial documents in your own inventories. For example, you can modify the query to be more specific, such as asking about the major drivers of revenue and profit growth or decline for a particular company. This approach helps to enhance the precision and relevance of the model's responses, enabling you to obtain more specific and insightful information from the financial statements.</p> <p>Let's revise this question and add more content specific prompt: What are the major drivers of revenue and profit growth or decline of this company?</p> <p> </p> <p>As we can see, the revised response provides more direct and relevant information tailored to the specific company.</p> Query Response What are the major drivers of revenue and profit growth or decline of this company? Major drivers of revenue and profit growth or decline of this company include seasonal holiday demand, new product and service introductions, demand for a single product, competitive global markets, economic conditions, and product pricing pressures. <p>Once you have a prompt that returns the best answer, you can update, delete, or add new prompts to your prompt inventories. This functionality allows you to conveniently manage and organize your prompt inventories, making it easy to access relevant materials and quickly retrieve queries that yield the best results. These capabilities save time and increase productivity in analyzing domain-specific documents.</p> <p>As you refine your prompt inventories, you can now apply your best prompts to completely new documents. In the following examples, you apply your improved prompt inventories to analyze Apple's financial document using enhanced prompts:</p> Query Response How does the company generate its revenue? What are its main products or services? The Company generates its revenue by selling its own products and services, as well as reselling third-party products, directly to consumers, small and mid-sized businesses, and education, enterprise and government customers through its retail and online stores and its direct sales force. Its main products and services include electronic devices and media content. Does the company operate in multiple geographic regions or industries? Yes, the company operates in multiple geographic regions, including the Americas, Europe, Greater China, Japan, and Rest of Asia Pacific. Are there any significant changes or developments in the company's business model, strategy, or competitive landscape? Yes, the company is subject to rapid change and the development of new business models and the entry of new and well-funded competitors. What were the company's revenues and net income for the past few years, and how have they changed over time? The company's revenues for 2020, 2021, and 2022 were $48.7 billion, $67.1 billion, and $48.7 billion respectively. The company's net income for 2020, 2021, and 2022 were $91.2 million, $108.9 million, and $119.4 million respectively. The company's revenues and net income have increased over time. What are the major drivers of revenue and profit growth or decline of this company? Major drivers of revenue and profit growth or decline of this company include seasonal holiday demand, new product and service introductions, demand for a single product, competitive global markets, economic conditions, and product pricing pressures. Are there any significant trends or patterns in the company's financial statements? It is not possible to answer this question without reviewing the financial statements. Who are the company's key executives and board members? What is their experience and track record? The company's key executives and board members are Jeffrey P. Bezos (Executive Chair), Andrew R. Jassy (President and Chief Executive Officer), Douglas J. Herrington (CEO Worldwide Amazon Stores), Keith B. Alexander (CEO, President, and Chair of IronNet, Inc.), Edith W. Cooper (Former Executive Vice President, Goldman Sachs Group, Inc.), Jamie S. Gorelick (Partner, Wilmer Cutler Pickering Hale and Dorr LLP), Daniel P. Huttenlocher (Dean, MIT Schwarzman College of Computing), Judith A. McGrath (Former Chair and CEO, MTV Networks), Indra K. Nooyi (Former Chief Executive Officer, PepsiCo, Inc.), Jonathan J. Rubinstein (Former co-CEO, Bridgewater Associates, LP), Patricia Q. Stonesifer (Former President and Chief Executive Officer, Martha's Table), and Wendell P. Weeks (Chief Executive Officer, Corning Incorporated). Their experience and track record can be found in the company's SEC filings. Are there any related-party transactions or potential conflicts of interest? No, there are no related-party transactions or potential conflicts of interest mentioned in the context. Does the company have effective internal controls and risk management procedures in place? It is not possible to answer this question definitively based on the given context. The context indicates that the company has internal controls and risk management procedures in place, but it is not clear if they are effective. What is the company's overall financial health, including its liquidity, solvency, and capital structure? It is difficult to say without more information. The context provided suggests that the company is exposed to a variety of risks that could affect its financial health, including changes in liquidity, credit deterioration, market and economic conditions, political risk, sovereign risk, and interest rate fluctuations. Additionally, the company has been adversely affected by the COVID-19 pandemic, which could have further impacts on its business, results of operations, financial condition, and stock price. How much debt does the company have, and what are the terms and conditions? The company has outstanding fixed-rate notes with varying maturities for an aggregate principal amount of 67,651 as of September 24, 2022, and 72,968 as of September 25, 2021. The notes are senior unsecured obligations and interest is payable in arrears. Are there any contingent liabilities or off-balance-sheet arrangements that could impact the company's financial position? Yes, there are commitments and contingencies mentioned in the context that could impact the company's financial position. <p>Once you're finished, you can easily download the resulting CSV file. Here's an excerpt of the actual result from the Apple Financial Statement chat. Please note that the displayed content has been truncated for brevity, but it provides a glimpse of the valuable information available to you.</p> Query Response Source Document Source Content How does the company generate its revenue? What are its main products or services? The Company generates its revenue by selling its own products and services, as well as reselling third-party products, directly to consumers, small and mid-sized businesses, and education, ... source_documents\\aapl-10-k.pdf \"The Company distributes its products and certain of its services through cellular network carriers, wholesalers, retailers and resellers, many of which distribute products and services from competitors ... \" Does the company operate in multiple geographic regions or industries? Yes, the company operates in multiple geographic regions, including the Americas, Europe, Greater China, Japan, and Rest of Asia Pacific. source_documents\\aapl-10-k.pdf \"The Company manages its business primarily on a geographic basis. The Company's reportable segments consist of the Americas, Europe, Greater China, Japan and Rest of Asia Pacific. Americas includes both North and South America. Europe includes European countries ...\""},{"location":"10-ks/10ksimpact.html","title":"10-Ks Impact","text":""},{"location":"10-ks/10ksimpact.html#impact","title":"Impact","text":"<p>At Anote, we are able to render our AI technology on a interactive user interface to help analysts extract information from 10-Ks:</p> <p></p> <p>As the user answers a question from one 10-K, we are able to incorporate feedback to provide better answers on the next 10-K.</p> <p></p> <p>We iterate on this process refining the answers to queries based on human feedback. This technology can have a massive impact, making analysis of 10-Ks more time efficient, less expensive in a scalable and accurate way, as seen below.</p>"},{"location":"10-ks/10ksimpact.html#time-efficiency","title":"Time Efficiency","text":"<p>The introduction of Anote's technology can analyze vast amounts of data 10 times faster than human analysts, equating to an annual saving of approximately 10,000 person-hours.</p>"},{"location":"10-ks/10ksimpact.html#cost-savings","title":"Cost Savings:","text":"<p>By reducing the labor hours associated with manual data extraction and analysis, the automated approach directly translates to massive cost savings. On average, the introduction of automation can save up to $500,000 per year.</p>"},{"location":"10-ks/10ksimpact.html#improved-accuracy","title":"Improved Accuracy","text":"<p>With this technology, one can obtain an accuracy rate of 88% in information extraction tasks. This level of precision is vital, especially considering the potentially major financial ramifications of inaccuracies within these intricate documents.</p>"},{"location":"10-ks/10ksimpact.html#scalability","title":"Scalability","text":"<p>As a company grows and the number of 10-K documents it needs to analyze increases, our system can handle larger volumes of data without compromising on speed or accuracy, making it a sustainable solution for businesses of all sizes.</p>"},{"location":"10-ks/10ksimpact.html#conclusion","title":"Conclusion","text":"<p>The use of Anote's technologies to decipher and extract information in 10-Ks provides a level of efficiency, accuracy, and cost-effectiveness that outpaces traditional manual analysis. This can help make the job of many financial analysts a lot easier.</p>"},{"location":"10-ks/10ksquestions.html","title":"10-Ks Questions","text":""},{"location":"10-ks/10ksquestions.html#common-questions-in-10-ks","title":"Common Questions in 10-Ks","text":"<p>Oftentimes, when analysts go through 10-Ks extracting information, there are key questions that they are looking to find the answers to. Here are some common questions of interest:</p> <p>Business Operations:</p> <ol> <li>How does the company generate its revenue? What are its main products or services?</li> <li>Does the company operate in multiple geographic regions or industries?</li> <li>Are there any significant changes or developments in the company's business model, strategy, or competitive landscape?</li> </ol> <p>Financial Performance:</p> <ol> <li>What were the company's revenues and net income for the past few years, and how have they changed over time?</li> <li>What are the major drivers of revenue and profit growth or decline?</li> <li>Are there any significant trends or patterns in the company's financial statements?</li> </ol> <p>Management and Governance:</p> <ol> <li>Who are the company's key executives and board members? What is their experience and track record?</li> <li>Are there any related-party transactions or potential conflicts of interest?</li> <li>Does the company have effective internal controls and risk management procedures in place?</li> </ol> <p>Financial Position:</p> <ol> <li>What is the company's overall financial health, including its liquidity, solvency, and capital structure?</li> <li>How much debt does the company have, and what are the terms and conditions?</li> <li>Are there any contingent liabilities or off-balance-sheet arrangements that could impact the company's financial position? --&gt;</li> </ol> <p>Risk Factors:</p> <ol> <li>What are the primary risks and uncertainties the company faces? How might they impact its future performance?</li> <li>Has the company identified any new risks or changed its risk profile since the previous filing?</li> <li>Are there any legal or regulatory issues that could affect the company's operations?</li> </ol> <p>Future Outlook:</p> <ol> <li>What are the company's strategic priorities and growth prospects?</li> <li>Are there any anticipated changes in the industry or market conditions that could affect the company?</li> <li>What are management's expectations and guidance for future financial performance?</li> </ol> <p>For a given 10-K for a specific company, this may be the following response from a LLM like GPT-3:</p> Query Response How does the company generate its revenue? What are its main products or services? The Company generates its revenue by selling its products and services directly to consumers, small and mid-sized businesses, and education, enterprise and government customers through its retail and online stores and its direct sales force. It also employs a variety of indirect distribution channels, such as third-party cellular network carriers, wholesalers, retailers, and resellers. The Company's main products and services include cellular network carriers, wholesalers, retailers, resellers, and other products and services. Does the company operate in multiple geographic regions or industries? Yes, the company operates in multiple geographic regions, including the Americas, Europe, Greater China, Japan, and Rest of Asia Pacific. Are there any significant changes or developments in the company's business model, strategy, or competitive landscape? No, the context does not mention any significant changes or developments in the company's business model, strategy, or competitive landscape. What were the company's revenues and net income for the past few years, and how have they changed over time? The company's revenues for 2022, 2021, and 2020 were $152,895 million, $137,006 million, and $91,240 million, respectively. The company's net income for 2022, 2021, and 2020 were $35,233 million, $32,505 million, and $22,170 million, respectively. The company's revenues and net income have increased over time. What are the major drivers of revenue and profit growth or decline? The major drivers of revenue and profit growth or decline are net sales, cost of sales, operating expenses, product pricing, competition, and gross margins. Are there any significant trends or patterns in the company's financial statements? No, the financial statements present fairly, in all material respects, the financial position of Apple Inc. at September 24, 2022, and September 25, 2021. There is no discussion of trends or patterns in the financial statements. Who are the company's key executives and board members? What is their experience and track record? The key executives and board members of the company are Alex Gorsky, Andrea Jung, Arthur D. Levinson, Monica Lozano, Ronald D. Sugar, and Susan L. Wagner. They have experience in consulting, analytics, mission operations, technology, systems delivery, cybersecurity, engineering, and innovation expertise. Are there any related-party transactions or potential conflicts of interest? It is not clear from the context if there are any related-party transactions or potential conflicts of interest. Does the company have effective internal controls and risk management procedures in place? It is not possible to answer this question definitively based on the given context. The context indicates that the company has internal controls and risk management procedures in place, but it is not clear if they are effective. What is the company's overall financial health, including its liquidity, solvency, and capital structure? It is difficult to say without more information. The context provided suggests that the company is exposed to a variety of risks that could affect its financial health, including changes in liquidity, credit deterioration, market and economic conditions, political risk, sovereign risk, and interest rate fluctuations. Additionally, the company has been adversely affected by the COVID-19 pandemic, which could have further impacts on its business, results of operations, financial condition, and stock price. How much debt does the company have, and what are the terms and conditions? The company has outstanding fixed-rate notes with varying maturities for an aggregate principal amount of $54,763 million. The remaining term in excess of one year as of September 24, 2022, is $13,488 million in 2023, $4,876 million in 2024, $1,418 million in 2025, $6,780 million in 2026, $312 million in 2027, and $412 million thereafter. The notes are senior unsecured obligations, and interest is payable in arrears. Are there any contingent liabilities or off-balance-sheet arrangements that could impact the company's financial position? Yes, there are commitments and contingencies mentioned in the context that could impact the company's financial position."},{"location":"10-ks/10kssolution.html","title":"10-Ks Solution","text":""},{"location":"10-ks/10kssolution.html#incorporating-human-feedback-for-prompting","title":"Incorporating Human Feedback for Prompting","text":"<p>Each 10-K document carries its unique information of characteristics and nuances, shaped by the company's industry, operations, and strategic direction. For instance, the Amazon 10-K may lay emphasis on its e-commerce and cloud computing operations, while Apple's report might be filled with details about its innovative consumer electronic devices, and Netflix may underline its content production and subscription-based streaming services.</p> <p>This inherent diversity means analysts, or AI models in this context, often have to reprompt or adjust their approach for each new document, fine-tuning the extraction process for better accuracy. This manual intervention can be time-consuming, reducing the overall efficiency of the data extraction process.</p> <p>But, what if there was a way to eliminate this repetitive cycle of reprompting and refactoring? What if there was a solution that could generalize the annotator feedback, allowing the model to learn and adapt from a few human interventions and apply this knowledge across all your 10-Ks?</p>"},{"location":"10-ks/10kssolution.html#the-anote-solution","title":"The Anote Solution","text":"<p>At Anote, you can start uploading your 10-K documents using our user-friendly interface shown below. Once the documents are uploaded, our powerful backend utilizes machine learning algorithms to extract the most relevant information in a matter of seconds. This information could include the title of the company, trading letters, current executives and key people, total assets, and revenue.</p> <p>However, extracting domain-specific information requires a more sophisticated approach than simple machine learning models. At Anote, we provide a solution where users can ask pre-defined domain-specific questions. For example, financial analysts might be interested in the following important questions:</p> <p>Financial Performance</p> <ol> <li>What were the company's revenues and net income for the past few years, and how have they changed over time?</li> <li>What are the major drivers of revenue and profit growth or decline?</li> <li>Are there any significant trends or patterns in the company's financial statements?</li> </ol> <p>Risk Factors</p> <ol> <li>What are the primary risks and uncertainties the company faces? How might they impact its future performance?</li> <li>Has the company identified any new risks or changed its risk profile since the previous filing?</li> <li>Are there any legal or regulatory issues that could affect the company's operations?</li> </ol> <p> </p> <p>Anote not only returns the best-fit answers to the questions but also provides information on the source file and source contents from which the response was formulated. Users can access the final outcome through the Anote interface and make changes if needed, or download the results for further analysis on their local machine.</p>"},{"location":"10-ks/10kssolution.html#model-fine-tuning-and-human-feedback","title":"Model Fine Tuning and Human Feedback","text":"<p>Large language models can make mistakes when operating under domain-specific conditions, such as analyzing 10-Ks. As we have seen before, when we ask a set of six questions, the model could make mistakes in 3 out of 6 cases. This is one of the challenging issues when analyzing 10-Ks. However, Anote leverages human feedback to fine-tune the large language model.</p> <p> </p> <p>In a nutshell, users can modify the output of the initial questions and provide feedback to the model. The model can be one of the large language models such as OpenAI, Llama, or GPT4All. The fine-tuned model learns from human feedback and improves its performance when analyzing the next set of documents.</p> <p>For example, when we ask the initial set of six questions using Apple's 10-K document, we may get correct answers for only 3 out of the 6 questions. The user can then correct questions 1, 3, and 5. The model receives human feedback and modifies its structure to identify the next set of 6 questions from the next document, such as Amazon's 10-K.</p> <p>As we can see from the Amazon example, the model now returns correct answers for questions 1, 3, and 5. However, it still returns wrong answers for questions 2, 4, and 6. In a similar manner, the user can correct these responses, and the model will be further fine-tuned.</p> <p>When the model is applied again to the next document, such as Microsoft's 10-K, it correctly answers 5 out of the 6 questions. As we continue to correct errors encountered during the analysis, we fine-tune the model, allowing it to learn from human feedback and improve its performance with each subsequent step.</p> <p>By labeling more outputs as we analyze more documents, the model can actively learn from human feedback and improve over time. After just a few human interventions, our model is able to pick up correct answers for many of the rest of the 10-Ks. This iterative process significantly enhances the accuracy and quality of the output, saving time for the user / analyst.</p>"},{"location":"api-overview/chat.html","title":"Chat","text":"<p>Send a message to the chatbot based on documents previously uploaded.</p> <p>Parameters:</p> Name Type Description Default <code>chat_id</code> <code>int</code> <p>The ID of the chat that has had documents uploaded.</p> required <code>message</code> <code>str</code> <p>The message to send to the chatbot.</p> required <code>finetuned_model_key</code> <code>str</code> <p>An optional custom model OpenAI key. If provided, the chatbot uses this finetuned model for the response.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>response</code> <code>dict</code> <p>The JSON response from the API containing <code>answer</code>, <code>message_id</code> and <code>sources</code> (which contains the document name and the relevant chunk).</p> <p>Sample usage: <pre><code>from anoteai import Anote\napi_key = 'INSERT_API_KEY_HERE'\nAnote = Anote(api_key, isPrivate=False) #You can select isPrivate=True if you want to use private models\nchat_id = 5\nresponse = privategpt.chat(chat_id, \"What is this paper about?\", finetuned_model_key=\"ft:gpt-35-turbo-0613:personal:anote:8DO8V2LB\")\n</code></pre></p> <p>Response is given as a JSON in this format: <pre><code>{\n'answer': 'The paper on classification performance is about utilizing few-shot and active learning to enhance artificial intelligence models.,\n'message_id': 10,\n'sources': [\n['doc2.pdf', 'To address this, we delve into few-shot and active learning, where our goal is to improve AI models with human feedback.'],\n['doc1.pdf', 'Improving Classification Performance With Human Feedback: \\n\\Label a few, we label the rest']\n]\n}\n</code></pre></p>"},{"location":"api-overview/evaluate.html","title":"Evaluate","text":"<p>Evaluate predictions on one or multiple documents/text.</p> <p>Parameters:</p> Name Type Description Default <code>message_id</code> <code>str</code> <p>The ID of the message associated with the uploaded documents. This ID is used to identify which set of documents/text predictions should be evaluated.</p> required <p>Returns:</p> Name Type Description <code>response</code> <code>dict</code> <p>A JSON response from the API, containing the evaluation results, <code>answer_relevancy</code>, <code>faithfulness</code> of the predictions on the specified message's documents or text. If the message ID is not provided or invalid, the function returns an error message indicating that a valid message ID is required for the operation.</p> <p>Sample usage: <pre><code>from anoteai import Anote\napi_key = 'INSERT_API_KEY_HERE'\nAnote = Anote(api_key, isPrivate=False) #You can select isPrivate=True if you want to use private models\nmessage_id = 5\nresponse = privategpt.evaluate(message_id)\n</code></pre></p> <p>Response is given as a JSON in this format: <pre><code>{\n'answer_relevancy': 0.88,\n'faithfulness': 1.0\n}\n</code></pre></p>"},{"location":"api-overview/finetune.html","title":"Fine Tune","text":"<p>Train or Fine Tune a model via supervised, unsupervised or rlhf fine tuning</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model - which is referenced in the model_ids table alongside the model_id.</p> required <code>model_type</code> <code>str</code> <p>Model that is used to do the fine tuning - could be \"FT-GPT\", \"Llama3\" for supervised, \"MLM\" for unsupervised.</p> <code>FTGPT</code> <code>fine_tuning_type</code> <code>str</code> <p>The type of fine tuning - could be \"supervised\", \"unsupervised\" or \"rlhf\"</p> required <code>x_train_csv</code> <code>DataFrame</code> <p>training data for fine tuning (string is csv of file path)- for each row could contain question or context entries</p> required <code>y_train_csv</code> <code>DataFrame</code> <p>training labels for fine tuning (string is csv of file path) - for each row could contain answer entries</p> required <code>initial_model_id</code> <code>str</code> <p>if rlhf fine tuning type, can add initial unsupervised model id from pretraining step for transfer learning</p> required <code>document_files</code> <code>list[str]</code> <p>A list of file paths to documents for document-based tasks for training. Example: ['path/to/file1.pdf', 'path/to/file2.pdf'].</p> required <p>Returns:</p> Name Type Description <code>response</code> <code>dict</code> <p>A JSON response from the API, including the <code>model_id</code> of the trained model.</p>"},{"location":"api-overview/overview.html","title":"Overview","text":"<p>The Private Chatbot API is a software developer kit designed for developers to leverage Private LLMs to upload documents, chat with them and evaluate the answers. There are 3 API endpoints:</p> <ul> <li> <p>Upload: Upload documents or specify a ticker for data retrieval and Q&amp;A.</p> </li> <li> <p>Chat: Send a message to the chatbot based on documents previously uploaded.</p> </li> <li> <p>Evaluate: Evaluate predictions on one or multiple documents/text.</p> </li> </ul> <p>If you are a first time user looking to get started, navigate to setup.md, if you are looking for applications of the technology navigate to capabilities or see our Private Chatbot developer cookbook.</p>"},{"location":"api-overview/predict.html","title":"Predict","text":"<p>Make predictions based on a fine tuned model or default model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model - which is referenced in the model_ids table alongside the model_id.</p> required <code>model_id</code> <code>str</code> <p>The model id that is the output of the train API call - enables fine tuned models to be called.</p> required <code>question_text</code> <code>str</code> <p>A string that contains the question.</p> required <code>context_text</code> <code>str</code> <p>A string that contains additional text that can be concatenated to the question, if needed.</p> <code>None</code> <code>document_files</code> <code>list[str]</code> <p>A list of file paths to documents for document-based tasks for predictions. Example: ['path/to/file1.pdf', 'path/to/file2.pdf'].</p> <code>None</code> <p>Returns:</p> Name Type Description <code>response</code> <code>dict</code> <p>A JSON response from the API, including the <code>results</code> of the predictions.</p>"},{"location":"api-overview/setup.html","title":"Overview","text":"<p>The Private Chatbot API is a software developer kit designed for developers to leverage Private LLMs to upload documents, chat with them and evaluate the answers. There are 3 API endpoints:</p> <ul> <li> <p>Upload: Upload documents or specify a ticker for data retrieval and Q&amp;A.</p> </li> <li> <p>Chat: Send a message to the chatbot based on documents previously uploaded.</p> </li> <li> <p>Evaluate: Evaluate predictions on one or multiple documents/text.</p> </li> </ul>"},{"location":"api-overview/setup.html#setup","title":"Setup","text":"<p>This setup guide is designed to help get your local development environment setup and send your first API request. Throughout this guide, you will learn:</p> <ul> <li>How to setup your Private Chatbot account</li> <li>How to install the Private Chatbot Python Pip Package</li> <li>How to send your first Private Chatbot API request</li> </ul>"},{"location":"api-overview/setup.html#step-1-account-setup","title":"Step 1: Account setup","text":"<p>First, create an Private Chatbot (https://Anote.ai/) account or sign in. Next, navigate to the API key page and \"Create new API key\". Copy the API key, and make sure to save this somewhere safe and do not share it with anyone.</p>"},{"location":"api-overview/setup.html#step-2-installing-python-pip-package","title":"Step 2: Installing Python Pip Package","text":"<p>To use the Private Chatbot Python library, you need to have Python installed. To ensure you have Python installed, navigate to your Terminal or Command line:</p> <ul> <li> <p>MacOS: Open Terminal: You can find it in the Applications folder or search for it using Spotlight (Command + Space).</p> </li> <li> <p>Windows: Open Command Prompt: You can find it by searching \"cmd\" in the start menu.</p> </li> </ul> <p>Next, enter <code>python3 --version</code> and then press enter, to ensure you see a python version &gt;3.7.</p> <p>To install the Private Chatbot Python library from the terminal / command line, run: <pre><code>pip install -U Anote\n</code></pre></p>"},{"location":"api-overview/setup.html#step-3-sending-your-first-api-request","title":"Step 3: Sending your first API request","text":"<p>After you have Python configured and set up an API key, the final step is to send a request to the Private Chatbot API using the Python library. To do this, create a file named Anote-test.py using the terminal or an IDE. Download these documents and move them to your local directory.</p> <p>Now, you have two choices: you can either use the private models (LLaMa or Mistral), or you can use the public models (GPT-4 or Claude).</p> <p>If you want to use private models, inside the file, copy and paste the example below: <pre><code>from anoteai import Anote\napi_key = 'INSERT_API_KEY_HERE'\nAnote = Anote(api_key, isPrivate=True)\nfile_paths = ['doc1.pdf', 'doc2.pdf']\nupload_result = Anote.upload(task_type=\"documents\", model_type=\"llama\", file_paths=file_paths)\nprint(\"output from upload: \", upload_result)\nchat_id = upload_result['id']\nchat_result = Anote.chat(chat_id, \"What is this paper classification performance about?\")\nprint(\"output from chat: \", chat_result)\nmessage_id = chat_result['message_id']\nprint(\"output from evaluate:\", Anote.evaluate(message_id))\n</code></pre></p> <p>If you want to use public models, inside the file, copy and paste the example below: <pre><code>from anoteai import Anote\napi_key = 'INSERT_API_KEY_HERE'\nAnote = Anote(api_key, isPrivate=False)\nfile_paths = ['doc1.pdf', 'doc2.pdf']\nupload_result = Anote.upload(task_type=\"documents\", model_type=\"gpt\", file_paths=file_paths)\nprint(\"output from upload: \", upload_result)\nchat_id = upload_result['id']\nchat_result = Anote.chat(chat_id, \"What is this paper classification performance about?\")\nprint(\"output from chat: \", chat_result)\nmessage_id = chat_result['message_id']\nprint(\"output from evaluate:\", Anote.evaluate(message_id))\n</code></pre></p> <p>If you want to use both private and public models, inside the file, copy and paste the example below: <pre><code>from anoteai import Anote\napi_key = 'INSERT_API_KEY_HERE'\nAnote_private = Anote(api_key, isPrivate=True)\nAnote_public= Anote(api_key, isPrivate=False)\nfile_paths = ['doc1.pdf', 'doc2.pdf']\nupload_result = Anote_private.upload(task_type=\"documents\", model_type=\"llama\", file_paths=file_paths)\nprint(\"output from upload private: \", upload_result)\nchat_id = upload_result['id']\nchat_result = Anote_private.chat(chat_id, \"What is this paper classification performance about?\")\nprint(\"output from chat private: \", chat_result)\nmessage_id = chat_result['message_id']\nprint(\"output from evaluate private:\", Anote_private.evaluate(message_id))\nupload_result = Anote_public.upload(task_type=\"documents\", model_type=\"gpt\", file_paths=file_paths)\nprint(\"output from upload public: \", upload_result)\nchat_id = upload_result['id']\nchat_result = Anote_public.chat(chat_id, \"What is this paper classification performance about?\")\nprint(\"output from chat public: \", chat_result)\nmessage_id = chat_result['message_id']\nprint(\"output from evaluate public:\", Anote_public.evaluate(message_id))\n</code></pre></p> <p>To run the code, enter <code>python Anote-test.py</code> into the terminal. The output should look as follows (may differ slightly depending on whether you have selected private/public):</p> <pre><code>output from upload:  {'id': 49}\noutput from chat:  {'answer': 'This paper, \"Improving Classification Performance With Human Feedback\", is about enhancing the accuracy, recall, and precision of AI models with the help of human feedback', 'message_id': 87, 'sources': ['doc1.pdf', 'Improving Classification Performance With Human Feedback:\\n\\nLabel a few, we label the rest\\n\\nEden Chung, Liang Zhang, Katherine Jijo, Thomas Clifford, Natan Vidra\\n\\nAbstract\\n\\nIn the realm of artificial intelligence, where a vast majority of data is unstructured']}\noutput from evaluate: {'answer_relevancy': 0.887419956065742, 'faithfulness': 1.0}\n</code></pre>"},{"location":"api-overview/upload-private.html","title":"Upload - Private","text":"<p>Upload documents or specify a ticker for data retrieval and Q&amp;A. This method supports various tasks, such as uploading documents or querying the government's EDGAR database.</p> <p>Parameters:</p> Name Type Description Default <code>task_type</code> <code>str</code> <p>Specifies the type of task to perform. This can be document-based interaction (e.g., 'documents') or financial data analysis ('edgar').</p> required <code>model_type</code> <code>str</code> <p>Determines the AI model to use for processing the request. Different model types available are 'llama' for LLaMa2 and 'mistral' for Mistral.</p> required <code>ticker</code> <code>str</code> <p>The ticker symbol for financial data analysis tasks. Required if the task_type is 'edgar'. Example: 'AAPL' for Apple Inc.</p> <code>None</code> <code>file_paths</code> <code>list[str]</code> <p>A list of file paths to documents for document-based tasks. Required if task_type is 'documents'. Example: ['path/to/file1.pdf', 'path/to/file2.pdf'].</p> <code>None</code> <p>Returns:</p> Name Type Description <code>response</code> <code>dict</code> <p>A JSON response from the API, including the <code>chat_id</code> for interactions based on the uploaded content or specified ticker.</p> <p>Sample usage for uploading documents: <pre><code>from anoteai import Anote\napi_key = 'INSERT_API_KEY_HERE'\nAnote = Anote(api_key, isPrivate=True)\nfile_paths = ['doc1.pdf', 'doc2.pdf']\nresponse = Anote.upload(task_type=\"documents\", model_type=\"llama\", file_paths=file_paths)\n</code></pre></p> <p>Response is given as a JSON in this format: <pre><code>{\n\"id\": 5\n}\n</code></pre></p> <p>Sample usage for using Edgar: <pre><code>from anoteai import Anote\napi_key = 'INSERT_API_KEY_HERE'\nAnote = Anote(api_key, isPrivate=True)\nresponse = Anote.upload(task_type=\"edgar\", model_type=\"llama\", ticker=\"aapl\")\n</code></pre> Response is given as a JSON in this format: <pre><code>{\n\"id\": 5\n}\n</code></pre></p>"},{"location":"api-overview/upload.html","title":"Upload - Public","text":"<p>Upload documents or specify a ticker for data retrieval and Q&amp;A. This method supports various tasks, such as uploading documents or querying the government's EDGAR database.</p> <p>Parameters:</p> Name Type Description Default <code>task_type</code> <code>str</code> <p>Specifies the type of task to perform. This can be document-based interaction (e.g., 'documents') or financial data analysis ('edgar').</p> required <code>model_type</code> <code>str</code> <p>Determines the AI model to use for processing the request. Different model types available are 'gpt' for GPT-4 and 'claude' for Claude.</p> required <code>ticker</code> <code>str</code> <p>The ticker symbol for financial data analysis tasks. Required if the task_type is 'edgar'. Example: 'AAPL' for Apple Inc.</p> <code>None</code> <code>file_paths</code> <code>list[str]</code> <p>A list of file paths to documents for document-based tasks. Required if task_type is 'documents'. Example: ['path/to/file1.pdf', 'path/to/file2.pdf'].</p> <code>None</code> <p>Returns:</p> Name Type Description <code>response</code> <code>dict</code> <p>A JSON response from the API, including the <code>chat_id</code> for interactions based on the uploaded content or specified ticker.</p> <p>Sample usage for uploading documents: <pre><code>from anoteai import Anote\napi_key = 'INSERT_API_KEY_HERE'\nAnote = Anote(api_key, isPrivate=False)\nfile_paths = ['doc1.pdf', 'doc2.pdf']\nresponse = Anote.upload(task_type=\"documents\", model_type=\"gpt\", file_paths=file_paths)\n</code></pre></p> <p>Response is given as a JSON in this format: <pre><code>{\n\"id\": 5\n}\n</code></pre></p> <p>Sample usage for using Edgar: <pre><code>from anoteai import Anote\napi_key = 'INSERT_API_KEY_HERE'\nAnote = Anote(api_key, isPrivate=False)\nresponse = Anote.upload(task_type=\"edgar\", model_type=\"claude\", ticker=\"aapl\")\n</code></pre> Response is given as a JSON in this format: <pre><code>{\n\"id\": 5\n}\n</code></pre></p>"},{"location":"api-prompting/example1.html","title":"Baseline Models","text":"<p>To start, we evaluate baseline models on the Bizbench test dataset. There are 2 public baseline models, GPT and Claude, as well as 2 private baseline models, Llama3 and Mistral.</p>"},{"location":"api-prompting/example1.html#loading-test-dataset","title":"Loading Test Dataset","text":"<pre><code>from anoteai import Anote\napi_key = 'INSERT_API_KEY_HERE'\nAnote = Anote(api_key, isPrivate=False)\ntest_df = pd.read_csv(\"Bizbench.csv\")\n</code></pre>"},{"location":"api-prompting/example1.html#public-baseline-models","title":"Public Baseline Models","text":""},{"location":"api-prompting/example1.html#gpt-4o-mini","title":"gpt-4o-mini","text":"<pre><code>for i, row in test_df.iterrows():\n    row[\"gpt4_answer\"], row[\"gpt4_chunk\"] = Anote.predict(\n        model_name=\"raw_gpt4\",\n        model_type=\"gpt-4\",\n        question_text=row[\"question\"],\n        context_text=row[\"context\"]\n    )\n\ntest_df[[\"id\", \"gpt4_answer\"]].to_csv(\"raw_gpt4_submission.csv\")\n</code></pre>"},{"location":"api-prompting/example1.html#claude","title":"Claude","text":"<pre><code>for i, row in test_df.iterrows():\nrow[\"claude_answer\"], row[\"claude_chunk\"] = Anote.predict(\nmodel_name=\"raw_claude_opus\",\nmodel_type=\"claude\",\nquestion_text=row[\"question\"],\ncontext_text=row[\"context\"]\n)\ntest_df[[\"id\", \"claude_answer\"]].to_csv(\"raw_claude_submission.csv\")\n</code></pre>"},{"location":"api-prompting/example1.html#private-baseline-models","title":"Private Baseline Models","text":""},{"location":"api-prompting/example1.html#llama3","title":"Llama3","text":"<pre><code>for i, row in test_df.iterrows():\nrow[\"llama3_answer\"], row[\"llama3_chunk\"] = Anote.predict(\nmodel_name=\"raw_llama3\",\nmodel_type=\"llama3\",\nquestion_text=row[\"question\"],\ncontext_text=row[\"context\"]\n)\ntest_df[[\"id\", \"llama3_answer\"]].to_csv(\"raw_llama3_submission.csv\")\n</code></pre>"},{"location":"api-prompting/example1.html#mistral","title":"Mistral","text":"<pre><code>for i, row in test_df.iterrows():\nrow[\"mistral_answer\"], row[\"mistral_chunk\"] = Anote.predict(\nmodel_name=\"raw_mistral\",\nmodel_type=\"mistral\",\nquestion_text=row[\"question\"],\ncontext_text=row[\"context\"]\n)\ntest_df[[\"id\", \"mistral_answer\"]].to_csv(\"raw_mistral_submission.csv\")\n</code></pre>"},{"location":"api-prompting/example2.html","title":"Supervised Fine Tuning","text":"<p>To improve from the baseline model performance, we will do supervised fine tuning on targeted questions and answers within the financial domain, that way the model can better serve analysts and industry professionals who require precise and reliable answers to questions.</p>"},{"location":"api-prompting/example2.html#training-process","title":"Training Process","text":"<p>To do supervised Fine-Tuning, the process includes:</p> <ol> <li>Loading the FinanceBench dataset, a structured dataset that includes questions about financial statements / company performances, paired with corresponding answers / context.</li> <li>Extracting questions, answers, and contexts as training data, alongside the document links.</li> <li>Conducting supervised training with the Fine Tuned GPT library to enhance the model\u2019s accuracy and contextual understanding for financial texts.</li> </ol> <pre><code>from datasets import load_dataset\nfrom anoteai import Anote\napi_key = 'INSERT_API_KEY_HERE'\nAnote = Anote(api_key, isPrivate=False)\n# Traing Fine Tuned Model using FT-GPT on Financebench\ndataset = load_dataset(\"finance_bench\")\ntrain_df = dataset[\"train\"].to_pandas()\nx_train = train_df[\"question\"]\ny_train = train_df[[\"answer\", \"context\"]]\nfile_paths = list(train_df[\"doc_link\"])\nfine_tune_model_id = Anote.train(\nx_train_csv=x_train,\ny_train_csv=y_train,\nmodel_name=\"fine_tuned_gpt_on_financebench\",\nmodel_type=\"ft_gpt\",\nfine_tuning_type=\"supervised\",\nfile_paths=file_paths\n)['id']\n</code></pre>"},{"location":"api-prompting/example2.html#testing-the-model","title":"Testing the Model","text":"<p>After fine-tuning, the model is tested on a separate testing dataset, such as Bizbench, to evaluate its effectiveness in a practical scenario. This involves using the fine-tuned model to answer the test financial questions. <pre><code>test_df = pd.read_csv(\"Bizbench.csv\")\n# Test Fine Tuned GPT on Bizbench\nfor i, row in test_df.iterrows():\nrow[\"ft_gpt_answer\"], row[\"ft_gpt_chunk\"] = Anote.predict(\nmodel_name=\"fine_tuned_gpt\",\nmodel_id=fine_tuned_model_id,\nquestion_text=row[\"question\"],\ncontext_text=row[\"context\"]\n)\ntest_df[[\"id\", \"ft_gpt_answer\"]].to_csv(\"ft_gpt_submission.csv\")\n</code></pre></p>"},{"location":"api-prompting/example2.html#private-version-of-supervised-fine-tuning","title":"Private Version of Supervised Fine Tuning","text":"<p>We can do this same fine tuning process with Llama3 using QLORA for a private version of the supervised fine tuning process. <pre><code>from datasets import load_dataset\nfrom anoteai import Anote\napi_key = 'INSERT_API_KEY_HERE'\nAnote = Anote(api_key, isPrivate=False)\n# Traing Fine Tuned Model using Llama3 on Financebench\ndataset = load_dataset(\"finance_bench\")\ntrain_df = dataset[\"train\"].to_pandas()\nx_train = train_df[\"question\"]\ny_train = train_df[[\"answer\", \"context\"]]\nfile_paths = list(train_df[\"doc_link\"])\nfine_tune_model_id = Anote.train(\nx_train_csv=x_train,\ny_train_csv=y_train,\nmodel_name=\"ft_llama3_on_financebench\",\nmodel_type=\"llama3\",\nfine_tuning_type=\"supervised\",\nfile_paths=file_paths\n)['id']\ntest_df = pd.read_csv(\"Bizbench.csv\")\n# Fine Tuned QLORA with Llama3 on Bizbench\nfor i, row in test_df.iterrows():\nrow[\"ft_qlora_answer\"], row[\"ft_qlora_chunk\"] = Anote.predict(\nmodel_name=\"fine_tuned_qlora\",\nmodel_id=fine_tuned_model_id,\nquestion_text=row[\"question\"],\ncontext_text=row[\"context\"]\n)\ntest_df[[\"id\", \"ft_qlora_answer\"]].to_csv(\"ft_qlora_submission.csv\")\n</code></pre></p>"},{"location":"api-prompting/example3.html","title":"Supervised Fine Tuning Methods","text":"<p>Supervised Fine-tuning is when the training data consists of paired input and outputs of various examples that demonstrate how you want the model to respond to certain queries. By fine-tuning the model, we aim to get the model to mimic the style of the training data when asked questions similar to those in the data. Since our goal for the fine-tuned model was to be good at answering questions on financial documents, our training data consisted of a set of questions and answers curated by human financial experts. The datasets we employed were meant for supervised fine-tuning as they consisted of a question, context, and answer column.</p>"},{"location":"api-prompting/example3.html#parameter-efficient-fine-tuning","title":"Parameter Efficient Fine Tuning","text":"<p>Parameter Efficient Fine Tuning (PEFT) is a method that significantly reduces the amount of compute and memory required for Fine Tuning LLMs. Instead of retraining and adjusting all the weights, PEFT freezes all the weights of the pre-trained model and then augments it with additional parameters during the fine tuning process. This differs from full fine tuning which retrains all the parameter weights, and transfer learning which only retrains the head of the model. PEFT can result in comparable performance to fully fine tuned models while having significantly less trainable parameters. It also reduces the risk of a model forgetting a lot of core material and enables the same base model to be fine tuned for various different use cases since the all the base pre-trained weights remain fixed.</p> <p></p>"},{"location":"api-prompting/example3.html#lora","title":"LORA","text":"<p>Low Rank Adaptation (LoRA) is a method within PEFT that further reduces the number of trainable parameters through the use of low-rank matrices. Instead of representing the weights in a d x d matrix for example, LoRA represents it as the product of (d x r)(r x d), where r is a rank significantly lower than d. Compared to full fine tuning, LoRA uses 10000 times less trainable parameters and three times less memory. 5 PEFT with LoRA is significantly less computationally expensive that full fine tuning and has been proven to result in comparable performance, as seen in Figure 2. Therefore, such techniques have been widely adapted and the ability to fine tune LLMs for specific use cases has been dramatically democratized.</p> <p></p>"},{"location":"api-prompting/example3.html#qlora","title":"QLORA","text":"<p>QLoRA, or Quantized Low-Rank Adaptation is a method that further increases the efficiency of fine tuning by significantly reducing the memory required while preserving performance through the use of quantization. Tensors in these models are usually in 16-bit precision and a quantized model then reduces the precision and memory of these models, usually to 4-bit precision. QLoRA quantizes each input tensor of the neural network and backpropagates gradients through a this quantized model into Low Rank Adapters</p>"},{"location":"api-prompting/example4.html","title":"Unsupervised Fine Tuning","text":"<p>Doing unsupervised learning to train a Masked Language Model (MLM) on financial documents, specifically 10-K filings from the SEC Edgar database, can enhance the model's understanding of financial terminology and contexts. Unsupervised fine-tuning allows the model to learn from a large corpus of unlabelled financial documents, improving its ability to understand and generate text related to financial topics.</p>"},{"location":"api-prompting/example4.html#data-preparation","title":"Data Preparation:","text":"<p>Collect URLs and file paths of 10-K filings. Example sources include the SEC Edgar website and local storage for PDF files.</p>"},{"location":"api-prompting/example4.html#continual-pre-training","title":"Continual Pre-Training:","text":"<p>Unsupervised fine tuning leverages continual pre-training, which involves updating a pre-trained language model by training it incrementally on new, domain-specific data without forgetting previously learned information.</p> <pre><code>from anoteai import Anote\napi_key = 'INSERT_API_KEY_HERE'\nAnote = Anote(api_key, isPrivate=False)\nfile_paths = [\n'https://www.sec.gov/Archives/edgar/data/320193/000032019318000145/a10-k20189292018.htm',\n'https://www.sec.gov/Archives/edgar/data/320193/000119312514383437/d783162d10k.htm',\n'10-Ks/aapl-10-k.pdf', '10-Ks/amzn-20221231.pdf', '10-Ks/bankofamerica-10K.pdf',\n'10-Ks/dbx-20221231.pdf', '10-Ks/google-10-k.pdf,', '10-Ks/msft-10k_20200630.pdf', '10-Ks/nflx-20221231.pdf',\n'10-Ks/nvda-10-k.pdf', '10-Ks/path-20230131.pdf', '10-Ks/sstk-20221231.pdf'\n]\nfine_tune_model_id = Anote.train(\nmodel_name=\"fine_tuned_mlm_on_10ks\",\nmodel_type=\"MLM\",\nfine_tuning_type=\"unsupervised\",\ndocument_files=file_paths\n)['id']\n</code></pre>"},{"location":"api-prompting/example4.html#using-the-fine-tuned-model","title":"Using the Fine-Tuned Model","text":"<p>Once the model is fine-tuned, it can be used to perform specific NLP tasks such as answering questions or generating text based on the financial context.</p> <pre><code>import pandas as pd\ntest_df = pd.read_csv(\"Bizbench.csv\")\n# Fine Tuned Masked Language Model\nfor i, row in test_df.iterrows():\nrow[\"ft_mlm_answer\"], row[\"ft_mlm_chunk\"] = Anote.predict(\nmodel_name=\"fine_tuned_mlm\",\nmodel_id=fine_tuned_model_id,\nquestion_text=row[\"question\"],\ncontext_text=row[\"context\"]\n)\ntest_df[[\"id\", \"ft_mlm_answer\"]].to_csv(\"ft_mlm_submission.csv\")\n</code></pre>"},{"location":"api-prompting/example4.html#additional-resources","title":"Additional Resources","text":"<ul> <li>Continual Learning of Natural Language Processing Tasks: A Survey</li> <li>Contrinual Pre-training of Language Models</li> <li>Using Hugging Face transformers for MLM</li> <li>Continual Pretraining Example</li> <li>Fine-tuning a Pre-trained LLM with an Unlabelled Dataset</li> </ul>"},{"location":"api-prompting/example5.html","title":"Reinforcement Learning from Human Feedback","text":"<p>Reinforcement Learning from Human Feedback (RLHF) can be used to fine-tune financial models, particularly focusing on incorporating human or AI feedback effectively. The process involves using a pre-trained model and enhancing its capabilities through targeted training on a small set of labeled examples.</p>"},{"location":"api-prompting/example5.html#loading-the-pre-trained-model","title":"Loading the Pre-trained Model","text":"<p>Start by loading a model that has been previously fine-tuned on financial documents to ensure it has a foundational understanding of financial contexts.</p> <pre><code>from anoteai import Anote\napi_key = 'INSERT_API_KEY_HERE'\nAnote = Anote(api_key, isPrivate=False)\nfine_tune_model_id = Anote.load_model(\nmodel_name=\"fine_tuned_mlm_on_10ks\",\n)['id']\n</code></pre>"},{"location":"api-prompting/example5.html#incremental-training","title":"Incremental Training","text":"<p>The RLHF approach involves incrementally training the model by:</p> <ul> <li>Utilizing labeled examples from a dataset specifically designed for instruction following, such as llmware/rag_instruct_benchmark_tester.</li> <li>Conducting training in stages, where each stage refines the model\u2019s understanding based on new sets of feedback.</li> </ul>"},{"location":"api-prompting/example5.html#reinforcement-learning-from-human-and-ai-feedback","title":"Reinforcement Learning from Human and AI Feedback","text":"<p>Utilize both human and AI-generated feedback to fine-tune models:</p> <ul> <li>Human Feedback: Collect demonstrationsof model outputs from human labelers.</li> <li>AI Feedback: Use AI to simulate potential improvements for the most effective changes.</li> </ul> <p>If you are interested in RLAIF see our blog post.</p>"},{"location":"api-prompting/example5.html#training-process","title":"Training Process","text":"<p>Fine-tuning is conducted by training a reward model on the dataset where labelers have indicated preferred outputs. This model then serves as a reward function for fine-tuning the primary model using algorithms like Proximal Policy Optimization (PPO) or Direct Policy Optimization (DPO).</p>"},{"location":"api-prompting/example5.html#using-the-fine-tuned-model","title":"Using the Fine-Tuned Model","text":"<p>After the model has been enhanced through RLHF, it can be deployed to handle more complex financial queries and provide outputs that align closely with human expectations. By integrating human insights and AI enhancements, financial models can be significantly improved to reduce errors like hallucination and to perform better even with fewer parameters.</p> <pre><code># Training Fine Tuned Model using FT-GPT on Financebench\ndataset = load_dataset(\"llmware/rag_instruct_benchmark_tester\")\ntrain_df = dataset[\"train\"].to_pandas()\nn_increments = 500\nfor increment in range(n_increments / 5, n_increments, n_increments % 5):\nx_train = train_df[\"question\"]\ny_train = train_df[[\"answer\", \"context\"]]\nfile_paths = list(train_df[\"doc_link\"])\nfine_tune_model_id = Anote.train(\nx_train_csv=x_train,\ny_train_csv=y_train,\ninitial_model_id=fine_tuned_model_id\nmodel_name=\"rlhf_\" + str(increment) + \"_labels\",\nmodel_type=\"llama3\",\nfine_tuning_type=\"supervised\",\nfile_paths=file_paths\n)['id']\ntest_df = pd.read_csv(\"Bizbench.csv\")\n# Fine Tuned Masked Language Model\nfor i, row in test_df.iterrows():\nrow[\"rlhf_\" + str(increment) + \"_answer\"], \\\n            row[\"rlhf_\" + str(increment) + \"_chunk\"] = Anote.predict(\nmodel_name=\"rlhf_\" + str(increment) + \"_labels\",\nmodel_id=fine_tuned_model_id,\nquestion_text=row[\"question\"],\ncontext_text=row[\"context\"]\n)\ntest_df[[\"id\", \"rlhf_\" + str(increment) + \"_answer\"]].to_csv(\"rlhf_\" + str(increment) + \"_submission.csv\")\n</code></pre>"},{"location":"api-prompting/example5.html#additional-resources","title":"Additional Resources","text":"<ul> <li>Training Language Models to Follow Instructions with Human Feedback talks about OpenAI's research on RLHF and its applications to fine-tuning language models like GPT-3.</li> <li>Model Card for Instruction Following Model shares techniques for training smaller models to outperform larger ones through effective feedback utilization.</li> </ul>"},{"location":"api-prompting/example6.html","title":"Traditional RAG","text":""},{"location":"api-prompting/example6.html#retrieval-augmented-generation","title":"Retrieval Augmented Generation","text":"<p>Retrieval Augmented generation (RAG) emerges as a crucial process in optimizing the output of large language models. LLMs, with their vast training data and billions of parameters, excel at tasks like question answering, language translation, and sentence completion. However, inherent challenges include the generation of inaccurate or outdated responses, presenting false information, and a lack of adaptability to current events.</p> <p></p> <p>RAG addresses these issues by extending the capabilities of LLMs to reference authoritative knowledge bases outside their training data, enhancing relevance, accuracy, and usefulness in various contexts. RAG takes an input, retrieves a set of relevant/supporting documents from a source like for example your textbook pdf, and combines them with the original input prompt. This concatenated context is then fed to the text generator, producing the final output. This adaptability of RAG becomes valuable in situations where facts may change over time, a feature particularly useful as the parametric knowledge of LLMs remains static. RAG eliminates the need for retraining, allowing language models to access the latest information for generating reliable outputs through retrieval-based generation. introduced in Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</p>"},{"location":"api-prompting/example6.html#loading-testing-dataset","title":"Loading Testing Dataset","text":"<pre><code>from anoteai import Anote\napi_key = 'INSERT_API_KEY_HERE'\nAnote = Anote(api_key, isPrivate=False)\ntest_df = pd.read_csv(\"Bizbench.csv\")\n</code></pre>"},{"location":"api-prompting/example6.html#traditional-rag-public","title":"Traditional RAG Public","text":"<pre><code>for i, row in test_df.iterrows():\n    row[\"rag_answer\"], row[\"rag_chunk\"] = Anote.predict(\n        model_name=\"traditional_rag\",\n        model_type=\"rag\",\n        question_text=row[\"question\"],\n        context_text=row[\"context\"],\n    )\n\ntest_df[[\"id\", \"rag_answer\"]].to_csv(\"rag_submission.csv\")\n</code></pre>"},{"location":"api-prompting/example6.html#traditional-rag-private","title":"Traditional RAG Private","text":"<pre><code>from anoteai import Anote\napi_key = 'INSERT_API_KEY_HERE'\nAnote = Anote(api_key, isPrivate=True)\ntest_df = pd.read_csv(\"Bizbench.csv\")\nfor i, row in test_df.iterrows():\nrow[\"rag_private_answer\"], row[\"rag_private_chunk\"] = Anote.predict(\nmodel_name=\"traditional_rag_private\",\nmodel_type=\"rag_private\",\nquestion_text=row[\"question\"],\ncontext_text=row[\"context\"],\n)\ntest_df[[\"id\", \"rag_private_answer\"]].to_csv(\"rag_private_submission.csv\")\n</code></pre>"},{"location":"api-prompting/example6.html#rag-limitations","title":"RAG Limitations","text":"<p>The RAG model faces many limitations that impact its effectiveness in knowledge-intensive natural language processing tasks. Semantic search, a core component of RAG, exhibits challenges such as retrieving irrelevant or opposing information, indicating the model\u2019s sensitivity to language nuances and the potential for unexpected results. The ambiguity in understanding how the embedding model extracts and organizes information in vectors adds complexity to optimizing similarity functions. Additionally, the process of chunking, crucial in RAG, can result in information loss if not carefully designed. Below you can see four of the key limitations of RAG, and some of the proposed solutions:</p> <ul> <li>Items are located in different sections<ul> <li>For example, if you ask a model to extract all emails in this document, the answer could be on both page 1, page 6 and page 10. Because there are different chunks, retrieving more than one chunk of text and combining the answers can be difficult for RAG to do.</li> </ul> </li> <li>Similarity is not relevance<ul> <li>Just because it has the highest similarity to the prompt, doesn\u2019t mean that the chunk is most important or relevant.<ul> <li>HyDE (Hypothetical Document Embeddings): does query expansion so that rather than just querying the document based on the text, can have the LLM answer the question. From there, you can use FLARE add the LLM answer to the \u201cquestion\u201d in order to search results in the document.</li> <li>Reranking algorithms: uses NLP / ML to rank items based on a relevance score, pick the top k chunks to feed into the LLM as context for the answer</li> </ul> </li> </ul> </li> <li>Ineffective document chunking<ul> <li>For example, table or sentences can be cut off in half via naive or fixed size chunking, which is a problem unless context windows become very long. To address this, there are a few approaches.<ul> <li>recursive chunking: dynamically find right chunk / chunk_overlap to avoid cutting sentences off.</li> <li>element based chunking: tables, paragraphs, headers, sections each separate chunk</li> <li>metadata annotations: filtering, (by date, time, company), add summary, boolean operations</li> </ul> </li> </ul> </li> <li>Not domain specific<ul> <li>Do fine tuning instead of RAG, or fine tune the embedding algorithm</li> </ul> </li> </ul>"},{"location":"api-prompting/example7.html","title":"Enhancing RAG","text":""},{"location":"api-prompting/example7.html#metadata-annotations","title":"Metadata Annotations","text":"<p>When interacting with several different documents, there might be key data points within the metadata that standard retrieval algorithms completely miss out on. A struggle we had faced previously when trying to chat with multiple documents, is that original RAG pipelines would confuse chunks of different documents, so to overcome this, separate vector databases were created for each document. However, this is not practical for large-scale applications and if the user wants to switch between documents or chat with multiple simultaneously. Metadata annotations is a method to overcome this hurdle and enhance retrieval further.</p> <p>The nature of metadata annotations should be considered with the chunking strategies used. For example, since we will chunk the documents based on elements, details about each element can be included in the annotations. For example, if a table is created in a separate chunk, the metadata can include what kind of table it is (ex. income statement, cash flow statement, etc). Additionally, it is common to add summaries and representative keywords to metadata annotations for additional context. <pre><code>for i, row in test_df.iterrows():\nrow[\"metadata_annotations_answer\"], row[\"metadata_annotations_chunk\"] = Anote.predict(\nmodel_name=\"metadata_annotations\",\nmodel_type=\"metadata_annotations\",\nquestion_text=row[\"question\"],\ncontext_text=row[\"context\"]\n)\ntest_df[[\"id\", \"metadata_answer\"]].to_csv(\"metadata_submission.csv\")\n</code></pre></p>"},{"location":"api-prompting/example7.html#flare","title":"FLARE","text":"<p>Forward-looking active retrieval augmented generation (FLARE) addresses the pitfalls of traditional RAG techniques by employing a much more active approach rather than a one-and-done retrieval pipeline. Instead of doing a similarity search of just the query and document embeddings, FLARE prompts the LLM to generate a hypothetical response from the query without context, and then both the query and hypothetical response are used to trigger the RAG step to search for the most similar chunks. FLARE builds on top of this by setting a threshold when to and when to not trigger retrieval. Only if the next generated tokens have a probability under a certain threshold, meaning that the model is not confident in the next predictions, it will go and try to retrieve relevant chunks from the document. This way, the model does not have to rely on the initial chunks it retrieved but can actively update the context based on confidence. <pre><code>for i, row in test_df.iterrows():\nrow[\"flare_answer\"], row[\"flare_chunk\"] = Anote.predict(\nmodel_name=\"flare\",\nmodel_type=\"flare\",\nquestion_text=row[\"question\"],\ncontext_text=row[\"context\"]\n)\ntest_df[[\"id\", \"flare_answer\"]].to_csv(\"flare_submission.csv\")\n</code></pre></p>"},{"location":"api-prompting/example7.html#hyde","title":"HyDE","text":"<p>Hypothetical Document Embeddings (HyDE). With HyDE, instead of just doing a similarity search with just the user\u2019s original query, it uses an LLM to generate a theoretical document when responding to a query and then does the similarity search with both the original question and hypothetical answer. This technique has been shown to outperform standard retrievers and eliminate the need for custom embedding algorithms, but can occasionally lead to incorrect results as it is dependent on another LLM for additional context.  <pre><code>for i, row in test_df.iterrows():\nrow[\"hyde_answer\"], row[\"hyde_chunk\"] = Anote.predict(\nmodel_name=\"hyde\",\nmodel_type=\"hyde\",\nquestion_text=row[\"question\"],\ncontext_text=row[\"context\"]\n)\ntest_df[[\"id\", \"hyde_answer\"]].to_csv(\"hyde_submission.csv\")\n</code></pre></p>"},{"location":"api-prompting/example7.html#reranking","title":"Reranking","text":"<p>With standard RAG pipelines, one can specify the number of documents or chunks the algorithm should return and therefore how many chunks should be fed as context to the input query. Generally, the top 1 or 2 chunks are included as context, and these are the best results given a cosine similarity or k-nearest neighbors search. However, such algorithms will give the most similar chunks, which might not corre- spond to the most relevant chunks for context.</p> <p>Re-ranking algorithms is a method to prioritize the relevance over the similarity of the chunks. Essentially, a method like cosine similarity might rank the top 10 chunks, but a separate algorithm will re-rank the algorithms to be based on relevance, and then the top one or two chunks after the re-ranking will be augmented as context to the input query. Cohere\u2019s re-ranking algorithm is a popular one and it along with others uses additional machine learning and natural language processing techniques to further evaluate relevance beyond a similarity search. <pre><code>for i, row in test_df.iterrows():\nrow[\"reranking_answer\"], row[\"reranking_chunk\"] = Anote.predict(\nmodel_name=\"reranking\",\nmodel_type=\"reranking\",\nquestion_text=row[\"question\"],\ncontext_text=row[\"context\"]\n)\ntest_df[[\"id\", \"reranking_answer\"]].to_csv(\"reranking_submission.csv\")\n</code></pre></p>"},{"location":"api-prompting/example7.html#recursive-chunking","title":"Recursive Chunking","text":"<p>Recursive Chunking is an example of a more adaptable chunking strategy that uses other indicators and rules like punctuation to make chunking more dynamic. While the chunks will still be of relatively equal size, the additional parameters will ensure that the chunks are not cut mid-sentence, for example. Through using Python libraries like Spacy and NLTK, we can use more sophisticated sentence-spitting techniques that use natural language processing techniques to be more aware of the context of the document. <pre><code>for i, row in test_df.iterrows():\nrow[\"recursive_chunking_answer\"], row[\"recursive_chunking_chunk\"] = Anote.predict(\nmodel_name=\"recursive_chunking\",\nmodel_type=\"recursive_chunking\",\nquestion_text=row[\"question\"],\ncontext_text=row[\"context\"],\n)\ntest_df[[\"id\", \"recursive_chunking_answer\"]].to_csv(\"recursive_chunking_submission.csv\")\n</code></pre></p>"},{"location":"api-prompting/example7.html#element-based-chunking","title":"Element Based Chunking","text":"<p>When it comes to financial reports, these documents are generally quite long and contain more complicated structures like tables. Documents of the same nature, such as 10-ks also generally follow a specific format that can be indicated by the headings and subheadings of the documents. Due to the special nature of these financial reports, a version of element-based chunking that takes such facts into account could help to further improve retrieval. For instance if a title element is found, a new chunk is started and if a table element is found, a new chunk is started, preserving the entire table. <pre><code>for i, row in test_df.iterrows():\nrow[\"element_based_chunking_answer\"], row[\"element_based_chunking_chunk\"] = Anote.predict(\nmodel_name=\"element_based_chunking\",\nmodel_type=\"element_based_chunking\",\nquestion_text=row[\"question\"],\ncontext_text=row[\"context\"],\n)\ntest_df[[\"id\", \"element_based_chunking_answer\"]].to_csv(\"element_based_chunking_submission.csv\")\n</code></pre></p>"},{"location":"api-prompting/example8.html","title":"Evaluation","text":"<p>Evaluation metrics are important to measure how each fine tuned model performs versus raw traditional models such as GPT4 or Llama3. We use evaluation metrics understand and quantify performance of the fine tuned LLMs, to ensure they perform accurately.</p>"},{"location":"api-prompting/example8.html#structured-vs-unstructured-evaluation","title":"Structured vs. Unstructured Evaluation","text":"<p>When it comes to evaluating question and answering models on financial documents, the two main components are evaluating the model\u2019s ability to retrieve context as well as its ability to answer questions based on the context. Ideally, we have access to ground truth context and answers developed by human financial analysts for structured evaluation.</p> Question Human Answer Human Chunk Claude Model Answer Claude Model Chunk Llama Model Answer Llama Model Chunk What is the total amount of the invoice? $22,500.00 Total Amount $22,500.00 $22,500.00 total amount $22,500.00 $22,500.00 about Total Amount $22,500.00 What is the invoice number? #0001 INVOICE # 0001 #0001 about INVOICE # 0001 #0001 INVOICE # 0001 What is a list of the items being purchased? \u2022Front End Engineering Service; \u2022Back End Engineering Service; \u2022Quality Assurance Manager Front End Engineering Service $5000.00 Back End Engineering Service $7500.00 Quality Assurance Manager \u2022front end engineering service; \u2022back end engineering service; \u2022quality assurance manager FRONT END ENGINEERING SERVICE $5000.00 BACK END ENGINEERING SERVICE $7500.00 QUALITY ASSURANCE MANAGER \u2022front end engineering service; \u2022back end engineering service; \u2022quality assurance manager about Front End Engineering Service $5000.00 Back End Engineering Service $7500.00 Quality Assurance Manager What is the name of the contact for question? Bia Hermes contact Bia Hermes bia hermes about contact Bia Hermes Bia Hermes contact bia hermes What is the PO number? #1000 P.O. # 1000 about #1000 P.O. # 1000 #1000 p.o. # 1000 When is payment due? within 30 days of 01/01/2022 Payment is due within 30 days WITHIN 30 DAYS OF 01/01/2022 PAYMENT IS DUE WITHIN 30 DAYS WITHIN 30 DAYS OF 01/01/2022 Payment is due within 30 days <p>Notice how there is human labeled answers and chunks above, so we can compare each model's answers and chunks from Claude and Llama3 to the human labeled answer (which we take as ground truth). However, in scenarios where we do not have access to this ground truth data, we need metrics that inform us about the quality of the chunk and answer from the model, hence unstructured evaluation .</p> Question Claude Model Answer Claude Model Chunk Llama Model Answer Llama Model Chunk What is the total amount of the invoice? $22,500.00 total amount $22,500.00 $22,500.00 about Total Amount $22,500.00 What is the invoice number? #0001 about INVOICE # 0001 #0001 INVOICE # 0001 What is a list of the items being purchased? \u2022front end engineering service; \u2022back end engineering service; \u2022quality assurance manager FRONT END ENGINEERING SERVICE $5000.00 BACK END ENGINEERING SERVICE $7500.00 QUALITY ASSURANCE MANAGER \u2022front end engineering service; \u2022back end engineering service; \u2022quality assurance manager about Front End Engineering Service $5000.00 Back End Engineering Service $7500.00 Quality Assurance Manager What is the name of the contact for question? bia hermes about contact Bia Hermes Bia Hermes contact bia hermes What is the PO number? about #1000 P.O. # 1000 #1000 p.o. # 1000 When is payment due? WITHIN 30 DAYS OF 01/01/2022 PAYMENT IS DUE WITHIN 30 DAYS WITHIN 30 DAYS OF 01/01/2022 Payment is due within 30 days"},{"location":"api-prompting/example8.html#structured-evaluation-answer-accuracy","title":"Structured Evaluation Answer Accuracy","text":"<p>Answer accuracy takes in the model answer and ground truth answer to evaluate whether or not the model is correct or not. For questions asking for a particular numerical metric, we can use a binary evaluation via regex expressions to say whether it is correct or not. However for open ended, explanation based questions in natural language, it can be more complicated, so we can utilize the following metrics for evaluating answer accuracy:</p> Metrics Description Example of Calculation <code>LLM eval</code> This metric serves as a substitute for human evaluation, where we can prompt a model like GPT-4 to see if two answers have the same semantic meaning, and prompt it to assign a specific score Use GPT-4 to evaluate the semantic similarity between \"The sky is clear\" and \"It's a cloudless day\" and assign a score. <code>Cosine Similarity</code> This is a more automated way of comparing semantic meaning, however relies on both answers being extremely similar in order to have a high score Calculate the cosine similarity of the TF-IDF vectors for the sentences \"I enjoy reading books\" and \"Reading books is enjoyable\". <code>Rouge-L Score</code> This metric is based on the longest common subsequence (LCS) between our model output and reference Calculate the Rouge-L score by finding the LCS of \"The cat is sleeping on the mat\" and \"A cat sleeps on a mat\". <code>Bleu Score</code> This metric compares how similar two texts are as a number between 0 and 1. Generally a score of at least 0.6 means that two texts are similar enough to mean the same thing. Calculate the Bleu Score for machine translated text compared to a human reference translation to assess quality. <p>However, for the use case of answering questions on many long financial documents, oftentimes the answers to questions can be wrong or incorrect. Sometimes it is due to more answers from the model, but more often than not it is due to the model thinking the answer is in the wrong part of the document. Because of this, retrieval systems such as RAG focus on identifying the right source / chunk of text to better inform the answer, that way the answer is coming from the right section in the document.</p>"},{"location":"api-prompting/example8.html#retrieval-accuracy","title":"Retrieval Accuracy:","text":"<p>It is important to evaluate retrieval accuracy because if the chunk in the document that the model sources is incorrect, the answer to the question from the model is most likely going to be wrong.  Retrieval accuracy metric takes into account the context in addition to the answer as a way to evaluate how well the pipeline can retrieve the correct section. With access to the ground truth context, we can evaluate document level, page level and paragraph level, and multi-chunk level accuracies for retrieval.</p> Metrics Description <code>document level</code> This metric checks if retrieved chunk is on the same document in the document as the actual chunk <code>page level</code> This metric checks if retrieved chunk is on the same page in the document as the actual chunk <code>paragraph level</code> This metric checks if retrieved chunk is on the same paragraph in the document as the actual chunk <code>multi-chunk level</code> This metric checks if multiple retrieved chunk are found in the same place in the document as the actual chunks <p>Multi-chunk level accuracies is still a work in progress, and is very important for cases where answers to questions come from multiple chunks / pages / documents, where each chunk has contains an important part of information required for the answer. On the retrieval side, this is a limitation with RAG based systems that only find the top most similar chunk (where the similar chunk might not be the most relevant chunk).</p>"},{"location":"api-prompting/example8.html#unstructured-evaluation-metrics","title":"Unstructured Evaluation Metrics","text":"<p>Without access to ground truth answers, most of the evaluation metrics will be based on whether or not the model answer is grounded in the context retrieved. These metrics are from the RAGAS evaluation framework.</p> Metrics Description <code>Faithfulness</code> This metrics evaluates whether the answer is supported by the given context, and penalizes the model if it hallucinated information not supported by the text. <code>Answer Relevance</code> This metric evaluates whether or not the answer actually addresses the question. It does not account for accuracy, but penalizes for incomplete/redundant answers <p>These are the metrics currently supported from within our public chatbot SDK, as when you use a chatbot such as Private Chatbot, Perplexity, GPT, Bard or Claude, there is no source of ground truth for model answers to be compared against.</p>"},{"location":"api-prompting/example8.html#identifying-anomalies-and-measuring-trustworthiness","title":"Identifying Anomalies and Measuring Trustworthiness","text":"<p>Within the Anote documentation, we have resources in regards to identifying anomalies in answers to questions. We leverage cosine similarity to measure trustworthiness, to ensure the models output reliable results, and flag the rows where model answers appear to be incorrect.</p>"},{"location":"api-prompting/example8.html#aggregate-metrics","title":"Aggregate Metrics","text":"<p>In addition to understanding how each specific row metric performed for each specific question, answer, chunk, it's important to have aggregate metrics across the entire testing dataset. Aggregate metrics are important for benchmarking different fine tuned models across an entire testing data corpus. This could look something like the following.</p> Aggregate Evaluation Metrics Claude Open AI RLHF Model 10 Labels Unsupervised FT Model <code>Cosine Similarity Score</code> 0.778 0.821 0.621 .580 <code>Rouge-L Score</code> 0.824 0.901 0.780 .618 <code>LLM Evaluation Score</code> 0.802 0.821 0.650 .838 <p>What is important to note is that model performance is determined by the evaluation metrics specified, so the user should be able to add metrics that they care about for their specific domain to measure model performance.</p>"},{"location":"api-prompting/example8.html#financial-specific-metrics","title":"Financial Specific Metrics","text":"<p>For financial specific aggregate evaluation metrics, the end user might care about measuring Financial Domain Knowledge, Quantity Extraction and Program Synthesis. With Kensho Benchmarks, you can evaluate the performance of fine tuned models on the BizBench dataset with the following steps:</p> <ol> <li>Download CSVs of data from your Anote fine tuned models</li> <li>Navigate to Kensho Benchmarks website</li> <li>Sign into Kensho Benchmarks Portal</li> <li>Upload submitted CSVs into Kensho Benchmarks</li> <li>See model performance on the Kensho Benchmarks leaderboard.</li> </ol>"},{"location":"api-prompting/overview.html","title":"Overview","text":"<p>The goal is to measure how training and fine tuning question answering models improves the performance of the LLM.</p> <p>To do this, we evaluate a variety of models across 3 forms of fine tuning:</p> <ol> <li>Supervised Fine Tuning</li> <li>Unsupervised Fine Tuning</li> <li>RLHF Fine Tuning</li> </ol> <p>We also evaluate 5 forms of enhanced RAG methodologies:</p> <ol> <li>Metadata Filtering</li> <li>Reranking</li> <li>HyDE</li> <li>FLARE</li> <li>Recursive Chunking</li> </ol> <p>We compare the performance of these fine tuned models to the following baseline models:</p> <ol> <li>GPT-4</li> <li>Claude</li> <li>Llama3</li> <li>Mistral</li> </ol> <p>We evaluate these LLMs via our evaluation SDK, our evaluation dashboard, and within Kensho's AI Benchmark Tool. These results intend to concretly demonstrate the impact that training and fine-tuning LLMs have on improving model performance both for general LLMs, and within the financial domain specifically.</p>"},{"location":"api-prompting/overview.html#training-datasets-involved","title":"Training Datasets Involved","text":"<p>To conduct this research, we will leverage the following training datasets:</p>"},{"location":"api-prompting/overview.html#supervised-fine-tuning-dataset-financebench","title":"Supervised Fine Tuning Dataset - FinanceBench","text":"<p>FinanceBench is a novel benchmark developed by Patronus AI for evaluating the performance of LLMs on open-book financial question answering. The dataset is comprised of 10,231 questions about publicly traded companies. Each question is accompanied by corresponding answers and evidence strings. It covers 40 companies in the USA, spanning 361 public filings, including 10Ks, 10Qs, 8Ks, and Earnings Reports, released between 2015 and 2023. A typical entry contains the question (e.g., \u201cWhat Was AMCOR\u2019s Adjusted Non-GAAP EBITDA for FY 2023\u201d), the answer (e.g., \u201cAMCOR\u2019s Adj. EBITDA was 2,018 million USD in FY 2023\u201d), an evidence string (containing information needed to verify the answer), and a page number from the relevant document.</p> <p></p>"},{"location":"api-prompting/overview.html#unsupervised-fine-tuning-dataset-10-ks-from-edgar","title":"Unsupervised Fine Tuning Dataset - 10-Ks from EDGAR","text":"<p>Raw unstructured 10-K documents from the SEC EDGAR filings website.</p> <p></p>"},{"location":"api-prompting/overview.html#rlhf-dataset-rag-instruct","title":"RLHF Dataset - Rag Instruct","text":"<p>RAG Instruct Benchmark tester was designed by LLMWARE to measure the different capabilities of retrieval augmented generation in financial and legal enterprise use cases. The dataset includes 200 questions with context passages pulled from common \u2019retrieval scenarios\u2019, (e.g., financial news, earnings releases, contracts, invoices, technical articles, general news and short texts). The question span Core Q&amp;A Evaluation, Not Found Classification, Boolean - Yes/No, Math, Complex Q&amp;A and Summary categories.</p> <p></p>"},{"location":"api-prompting/overview.html#testing-datasets-involved","title":"Testing Datasets Involved","text":"<p>For our testing dataset, we will use BizBench, a benchmark for evaluating models' ability to reason about realistic financial problems. BizBench comprises eight quantitative reasoning tasks, focusing on question-answering (QA) over financial data via program synthesis. Test dataset includes financially-themed code-generation tasks from newly collected and augmented QA data, as well as the reasoning capabilities required for financial QA: reading comprehension of financial text and tables for extracting intermediate values, and understanding financial concepts and formulas needed to calculate complex solutions. Collectively, these tasks evaluate a model's financial background knowledge, ability to parse financial documents, and capacity to solve problems with code.</p>"},{"location":"api-qa/example1.html","title":"Asking questions from Apple's 10-K","text":"<pre><code>from anoteai import Anote\napi_key = 'INSERT_API_KEY_HERE'\nAnote = Anote(api_key, isPrivate=False) #You can select isPrivate=True if you want to use private models\nchat_id = Anote.upload(task_type=\"edgar\", model_type=\"gpt\", ticker=\"aapl\")['id']\nresponse = Anote.chat(chat_id, \"What does this company do?\")\nprint(response['answer'])\nmessage_id = response['message_id']\nprint(Anote.evaluate(message_id))\n</code></pre> <p>As an output we get: <pre><code>Apple is a technology company continually improves its\nproducts and services via researching and developing\nnew technologies hardware devices, accessories, software, and services.\n{'answer_relevancy': 0.8, 'faithfulness': 0.9}\n</code></pre></p>"},{"location":"api-qa/example2.html","title":"Asking questions from Uploaded Documents","text":"<pre><code>from anoteai import Anote\napi_key = 'INSERT_API_KEY_HERE'\nAnote = Anote(api_key, isPrivate=True) #You can select isPrivate=False if you want to use private models\nchat_id = Anote.upload(task_type=\"documents\", model_type=\"mistral\", file_paths=file_paths)['id']\nresponse = Anote.chat(chat_id, \"What does this company do?\")\nprint(response['answer'])\nprint(\"Sources:\", response['sources'])\nmessage_id = response['message_id']\nprint(Anote.evaluate(message_id))\n</code></pre> <p>As an output we get: <pre><code>The research paper \"Improving Classification Performance With Human Feedback\" is written by Eden Chung, Liang Zhang, Katherine Jijo, Thomas Clifford, and Natan Vidra.\nSources: [['Anote_research_paper.pdf', 'Improving Classification Performance With Human Feedback:\\n\\nLabel a few, we label the rest\\n\\nEden Chung, Liang Zhang, Katherine Jijo, Thomas Clifford, Natan Vidra\\n\\nAbstract\\n\\nIn the realm of artificial intelligence, where a vast majority of data is unstructured, obtaining sub-\\nstantial amounts of labeled data to train supervised machine learning models poses a significant\\nchallenge. To address this, we delve into few-shot and active learning, where are goal is to improve\\nAI models with human feedback on a few labeled examples. This paper focuses on understanding how\\na continuous feedback loop can refine models, thereby enhancing their accuracy, recall, and precision\\nthrough incremental human input. '], ['Anote_research_paper.pdf', 'By employing Large Language Models (LLMs) such as GPT-3.5,\\nBERT, and SetFit, we aim to analyze the efficacy of using a limited number of labeled examples to\\nsubstantially improve model accuracy. We benchmark this approach on the Financial Phrasebank,Banking, Craigslist, Trec, Amazon Reviews da']]\n{'answer_relevancy': 0.9307434918423216, 'faithfulness': 1.0}\n</code></pre></p>"},{"location":"api-qa/example3.html","title":"Uploading documents in different languages","text":"<pre><code>from anoteai import Anote\napi_key = 'INSERT_API_KEY_HERE'\nAnote = Anote(api_key, isPrivate=False) #You can select isPrivate=True if you want to use private models\nfile_paths = ['jap1.pdf', 'en1.pdf']\nchat_id = Anote.upload(task_type=\"documents\", model_type=\"claude\", file_paths=file_paths)['id']\nresponse_english = Anote.chat(chat_id, \"What is the Roy Hill Iron Ore Project?\")\nprint(response_english['answer'])\nprint(\"Sources:\", response_english['sources'])\nmessage_id_eng = response_english['message_id']\nprint(Anote.evaluate(message_id_eng))\nprint(\"-------------------------------------------------\")\nresponse_japanese = Anote.chat(chat_id, \"\u4f55\u3067\u3059\u304b\u30ed\u30a4\u30d2\u30eb\u9244\u9271\u5c71\u30d7\u30ed\u30b8\u30a7\u30af\u30c8?\")\nprint(response_japanese['answer'])\nprint(\"Sources:\", response_japanese['sources'])\nmessage_id_jap = response_japanese['message_id']\nprint(Anote.evaluate(message_id_jap))\n</code></pre> <p>As an output we get (condensed for brevity): <pre><code>The Roy Hill Iron Ore Project is an operation in Australia that actively introduces advanced technologies to streamline its processes, from iron ore exploration and mining to shipping at ports, enhancing its competitiveness...\nSources: [['en1.pdf', 'ss\\n\\nCase\u2460 \uff1a Australia \uff5c Roy Hill Iron Ore Project\\n\\n\u25b2\\n\\nRoy Hill iron ore project proactively applies cutting-edge technolo-\\ngies to seamlessly streamline their operation process, from iron ore \\nexploration/mining to shipping from ports, and to improve their com-\\npetitiveness.\u25b2\\n\\nThe operation center in Perth...'], ['en1.pdf, '(located 1,300 km away from the mine site)\u25b2\\n\\nAn unmanned, fully autonomous haulage system will soon be \\ndeployed for heavy dump trucks. This add-on system provides retro-\\nfit capabilities to automate dump trucks of different manufacturers. \\nThrough collaboration with a U.S. startup company']]\n{'answer_relevancy': 0.9534500905980047, 'faithfulness': 0.8571428571428571}\n-------------------------------------------------\n\u30ed\u30a4\u30d2\u30eb\u9244\u9271\u5c71\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306f\u3001\u8c6a\u5dde\u306b\u4f4d\u7f6e\u3057\u3066\u3044\u3066\u6700\u65b0\u6280\u8853\u306e\u7a4d\u6975\u7684\u306a\u5c0e\u5165\u306b\u3088\u308a\u3001\u9244\u9271\u77f3\u306e\u63a2\u67fb\u30fb\u63a1\u6398\u304b\u3089\u6e2f\u6e7e\u3067\u306e\u51fa\u8377\u307e\u3067\u3092\u30b7\u30fc\u30e0\u30ec\u30b9\u306b\u6700\u9069\u5316\u3057\u7af6\u4e89\u529b\u3092\u5f37\u5316\u3057\u3066\u3044\u307e\u3059\u3002\u30d1\u30fc\u30b9\u5e02\u306b\u3042\u308b\u30ea\u30e2\u30fc\u30c8\u30aa\u30da\u30ec\u30fc\u30b7\u30e7\u30f3\u30bb\u30f3\u30bf\u30fc\u3067\u3001\u9244\u9271\u77f3\u306e\u63a1\u6398\u304b\u3089\u6e2f\u6e7e\u3067\u306e\u8239\u7a4d\u307f\u307e\u3067\u3092\u30ea\u30a2\u30eb\u30bf\u30a4\u30e0\u3067\u9060\u9694\u7ba1\u7406\u3057\u3066\u3044\u307e...\nSources: [['jap1.pdf', '\\n\u8c6a\u5ddeRoy Hill\u9244\u9271\u5c71\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306b\u3066\u3001\u6700\u65b0\u6280\u8853\u306e\u7a4d\u6975\u5c0e\u5165\u306b\u3088\u308a\u3001\u9244\u9271\\n\u77f3\u306e\u63a2\u67fb\u30fb\u63a1\u6398\u304b\u3089\u6e2f\u6e7e\u3067\u306e\u51fa\u8377\u307e\u3067\u3092\u30b7\u30fc\u30e0\u30ec\u30b9\u306b\u6700\u9069\u5316\u3059\u308b\u3053\u3068\u3067\u7af6\\n\u4e89\u529b\u3092\u5f37\u5316\u3057\u3066\u3044\u308b\u3002\u25b2\\n\\n\u30d1\u30fc\u30b9...'], ['jap1.pdf', '\u3078\u306e\u5909\u8c8c\\n\\n\\n\\n15\\n\\nGC2021 DX\\nGlobal crossvalue platform\\n\\n\u9285\u9271\u5c71\u4e8b\u696d\u306e\u81ea\u52d5\u5316\u30fbDX\\n\\n\u4e8b\u4f8b\u2461 \uff1a \u30c1\u30ea \uff5c Antofagasta\u793e\u3068\u306e\u30c1\u30ea\u9285\u9271\u5c71\u4e8b\u696d\\n\\n\u25b2\\n\\nAntofagasta\u793e\u3068\u306e\u30c1\u30ea\u9285\u9271...']]\n{'answer_relevancy': 0.7301118502936456, 'faithfulness': 0.75}\n</code></pre></p> <p>Our model will be able to detect what language your question is in and find chunks from the relevant document accordingly.</p>"},{"location":"api-qa/example4.html","title":"Asking questions from websites","text":"<pre><code>from anoteai import Anote\napi_key = 'INSERT_API_KEY_HERE'\nAnote = Anote(api_key, isPrivate=False) #You can select isPrivate=True if you want to use private models\nfile_paths = ['https://docs.Anote.ai/', 'https://docs.Anote.ai/10-ks/10ksbackground.html', 'https://docs.Anote.ai/api-overview/setup.html'] #'https://Anote.ai/faqs', 'https://Anote.ai/research']\nchat_id = Anote.upload(task_type=\"documents\", model_type=\"gpt\", file_paths=file_paths)['id']\nresponse1 = Anote.chat(chat_id, \"What is Private Chatbot?\")\nprint(response1['answer'])\nprint(\"Sources:\", response1['sources'])\nmessage_id1 = response1['message_id']\nprint(Anote.evaluate(message_id1))\nprint(\"-------------------------------------------------\")\nresponse2 = Anote.chat(chat_id, \"Can you help me setup my local development environment?\")\nprint(response2['answer'])\nprint(\"Sources:\", response2['sources'])\nmessage_id2 = response2['message_id']\nprint(Anote.evaluate(message_id2))\n</code></pre> <p>As an output we get (condensed for brevity): <pre><code>Private Chatbot is a tool that enables enterprises to leverage generative AI and privacy preserving LLMs to engage with their documents while keeping their private data secure. It provides enterprises with their own AI assistant, that can answer any query based on their organizations' data. When members of the organization ask a question, the Private Chatbot uses a privacy-preserving retrieval component to search...\nSources: [['https://docs.Anote.ai/', \"cally on the user's device or local infrastructure. These models preserve user privacy by avoiding the transmission of user queries or documents to external servers.Private ChatsAll queries stay on your computer, never leaving your private, secure data silo. Private Chatbot ensures that user queries and responses are kept private. When the user asks a question...\"], ['https://docs.Anote.ai/', ' Table of contents        How is it Private?      Private Data      Private LLMs      Private Chats      How Does Private Chatbot Work?      Upload      Chat      Evaluate  Introduction to Private...']]\n{'answer_relevancy': 0.9709236021067721, 'faithfulness': 1.0}\n-------------------------------------------------\nYes, the text gives instructions on how to set up your local development environment. Here are the steps:\n\n1. Generate an API key for your local system and make sure to save it somewhere safe. Remember not to share it with anyone.\n\n2. Install Python in your system (if you don't have it yet). To check this, open your Terminal or Command line. For MacOS, find Terminal in the Applications folder or search for it using Spotlight (Command + Space). For Windows, open Command Prompt by searching \"cmd\" in the start menu. Verify your Python version by typing python3 --version and pressing enter. You should see a Python version higher than 3.7.\n\n3. Install the Private Chatbot Python Library. In the terminal or command line, type in pip install -U Anote and press enter.\n\n4. Sending your first API request. After configuring Python and setting up your API key, the next step is to send an API request to the Private Chatbot API using the Python library. To do this, create a file named Anote-test.py using the terminal or any Integrated Development Environment (IDE).\nSources: [['https://docs.Anote.ai/api-overview/setup.html', 'I key\". Copy the API key, and make sure to save this somewhere safe and do not share it with anyone.Step 2: Installing Python Pip PackageTo use the Private Chatbot Python library, you need to have Python installed. To ensure you have Python installed, navigate to your Terminal... '], ['https://docs.Anote.ai/', 'Docs']]\n{'answer_relevancy': 0.9084305015393235, 'faithfulness': 0.0}\n</code></pre></p>"},{"location":"api-qa/example5.html","title":"Asking questions from fine tuned model","text":"<pre><code>from anoteai import Anote\napi_key = 'INSERT_API_KEY_HERE'\nAnote = Anote(api_key, isPrivate=False) #You can select isPrivate=True if you want to use private models\nchat_id = Anote.upload(task_type=\"documents\", model_type=\"gpt\", file_paths=file_paths)['id']\nfile_paths = [\n'https://www.sec.gov/Archives/edgar/data/320193/000032019318000145/a10-k20189292018.htm',\n'https://www.sec.gov/Archives/edgar/data/320193/000119312514383437/d783162d10k.htm'\n]\nfine_tune_model_id = Anote.train(\nmodel_type=\"MLM\",\nfine_tuning_type=\"unsupervised\",\ndocument_files=file_paths\n)['id']\nresponse1 = Anote.chat(chat_id, \"What is Apple's Revenue in 2023?\" finetuned_model_key=fine_tuned_model_id)\nprint(response1['answer'])\nprint(\"Sources:\", response1['sources'])\nmessage_id1 = response1['message_id']\nprint(Anote.evaluate(message_id1))\n</code></pre>"},{"location":"privategpt/installation.html","title":"Installation","text":""},{"location":"privategpt/installation.html#step-1-download-anote","title":"Step 1. Download Anote","text":"<p>Download the Private Chatbot desktop app here</p> <p>Open the DMG on your Mac, and follow the on-screen instructions. It should look like below</p> <p></p>"},{"location":"privategpt/installation.html#step-2-download-llms","title":"Step 2. Download LLMs","text":"<p>Private Chatbot uses Ollama to manage the LLMs that it uses, LLaMa2 and Mistral.</p> <p>For Mac, download ollama here</p> <p>For Windows, download ollama here</p> <p>Then, download LLaMa2 and Mistral by opening your terminal and running</p> <p><code>ollama run llama2</code></p> <p><code>ollama run mistral</code></p>"},{"location":"privategpt/privategpt.html","title":"Introduction to Private Chatbot","text":"<p>Private Chatbot enables enterprises to leverage generative AI and privacy preserving LLMs to chat with their documents while keeping their private and data secure. For context, enterprises want to leverage Generative AI for analytics purposes, but have sensitive data that can not be shared off-premises to LLMs like ChatGPT. Enterprises want to leverage LLMs and Generative AI for analytics, while keeping their data private and secure.</p> <p>Private Chatbot provides enterprises with their own AI assistant, that can answer any query based on their organizations data. Members of the organization can chat with their documents to ask any question about their organization to obtain relevant insights, and are shown citations of where the answers specifically come from within documents in their enterprise. For enterprises, this can be viewed as an on premise GPT-for-your-business, where enterprises have their own GPT, catered specifically for their needs. Enterprises have no risk of sharing confidential and private data, as both the data and model is kept on premise, local, private and secure.</p>"},{"location":"privategpt/privategpt.html#how-is-it-private","title":"How is it Private?","text":"<p>Private Chatbot's paid version is completely private as data, models and chats are all run locally, on device via a desktop app, for enterprise grade security.</p>"},{"location":"privategpt/privategpt.html#private-data","title":"Private Data","text":"<ul> <li>Uploaded data is stored locally, on premise, for enterprise grade security: Private Chatbot operates entirely within the user's local environment, providing a secure and private space for document interactions. User documents are stored locally, ensuring that they remain secure on the user's device or local storage infrastructure.</li> </ul>"},{"location":"privategpt/privategpt.html#private-llms","title":"Private LLMs","text":"<ul> <li>LLMs like GPT4All and Llama2 run locally on your device. Private Chatbot employs privacy-aware large language models like LlamaCpp, Mistral and GPT4All, which operate locally on the user's device or local infrastructure. These models preserve user privacy by avoiding the transmission of user queries or documents to external servers.</li> </ul>"},{"location":"privategpt/privategpt.html#private-chats","title":"Private Chats","text":"<ul> <li>All queries stay on your computer, never leaving your private, secure data silo. Private Chatbot ensures that user queries and responses are kept private. When the user asks a question, Private Chatbot incorporates a privacy-preserving retrieval component that efficiently searches and retrieves relevant documents based on user queries. The retrieval process takes place locally, without transmitting sensitive information to external servers.</li> </ul> <p>By combining these principles, Private Chatbot empowers enterprises to chat with their documents in a privacy-preserving way, using the capabilities of generative AI models.</p>"},{"location":"privategpt/privategpt.html#how-does-private-chatbot-work","title":"How Does Private Chatbot Work?","text":"<p>To use Private Chatbot, there are 3 main steps:</p>"},{"location":"privategpt/privategpt.html#upload","title":"Upload","text":"<p>On Private Chatbot, you can either upload your own documents to a local silo, or connect to financial documents within the EDGAR API.</p> <p>Upload Documents: User can upload documents such as PDFs, DOCXs, TXTs, PPTXs, etc. locally from their computer.</p> <p>Query from EDGAR API: With the SEC's EDGAR API, users can enter a company ticker and retrieve relevant financial documents.</p>"},{"location":"privategpt/privategpt.html#chat","title":"Chat","text":"<p>Ask questions on your documents. Get answers and converse with your data. To mitigate hallucinations and ensure the answers are correct, we employ novel research techniques in fine tuning and retrieval augmented generation to provide more accurate answers as well as robust citations.</p> <p>Fine Tuning LLMs for Accurate Answers: To mitigate hallucinations, we leverage parameter efficient fine tuning with techniques such as LORA and Q-LORA for private LLMs such as Llama2. We have benchmarked LLMs with evaluation metrics such as rouge-l, LLM eval, cosine similarity and bleu score, and have shown that fine tuning significantly enhances the accuracy of the answers for your specific dataset.</p> <p>Enhanced Retrieval for Accurate Citations: To mitigate hallucinations, you can view the text from the specific document and page number where the model's answer came from. To ensure our model finds the right chunk for the citation, we have done a lot of research on how LLMs use retrieval augmented generation, and have benchmarked a variety of retrieval techniques such as HyDE and FLARE.</p>"},{"location":"privategpt/privategpt.html#evaluate","title":"Evaluate","text":"<p>View relevant citations to ensure the models answers are correct, where you can see the specific document, page number and chunk of text where the answer came from. Our team has implemented a variety of evaluation metrics within our developer docs, such as <code>cosine similarity</code>, <code>blue score</code>, <code>rouge-l score</code> for structured documents, as well as <code>llm eval</code> and <code>faithfulness</code> with ragas for unstructured documents, to accurate benchmark the performance of the LLMs. We evaluate LLMs both from the retrieval perspective of finding the right chunk for citations, as well as from the answer perspective of answering the question correctly.</p>"},{"location":"prompting/questionanswering.html","title":"Active Prompting using AI: Question and Answering","text":"<p>Question and Answering (Q&amp;A) prompting is a powerful technique that leverages AI to extract specific information from text documents by posing questions. This documentation provides a step-by-step guide to Q&amp;A prompting, including an example, a flowchart to visualize the process, and a table to demonstrate the data format.</p>"},{"location":"prompting/questionanswering.html#qa-prompting","title":"Q&amp;A Prompting","text":"<p>Using Anote, you can correct the answers generated by the language model based on your domain knowledge. This means that if you are working with legal documents, precision and accuracy are of utmost importance. With the interactive capability of the system, you can engage in a conversation with the documents and correct any inaccuracies or incorrect answers provided by the model.</p> <p></p> <p>As you actively correct the model's responses and provide the correct answers, the system learns from your inputs. The feedback you provide helps the model understand the nuances of the domain and improve its performance over time. With each interaction and correction you make, the model becomes better at returning accurate and relevant answers. This iterative process ensures that the model adapts and improves, leading to increased accuracy and enhanced performance in answering questions related to your specific domain.</p> <p></p>"},{"location":"prompting/questionanswering.html#steps-involved","title":"Steps Involved","text":"<p>After the successful loading of documents, the model proceeds with several steps that operate in the background to facilitate the Q&amp;A prompting process. These steps are designed to handle the text documents effectively and extract the necessary information to generate accurate responses. The flowchart below presents a visual representation of the steps involved in Q&amp;A prompting:</p> <p></p> <p>The steps involved are as follows:</p> <ol> <li> <p>Text Chunking: Initially, the text document is broken down into smaller, more manageable chunks. By decomposing the text, it becomes easier to analyze and match the content with specific prompts.</p> </li> <li> <p>Chunk Embedding: Each chunk of text is transformed into a numerical representation, typically using embedding techniques such as word embeddings or sentence embeddings. These representations capture the semantic meaning and context of the text chunks.</p> </li> <li> <p>Prompt Embedding: The user's question, known as the prompt, is also transformed into a numerical representation using the same embedding techniques employed in step 2. This enables a meaningful comparison between the prompt and the text chunks.</p> </li> <li> <p>Similarity Matching: The embedded prompt is compared to the embedded text chunks to identify the most similar chunk. This comparison is usually done by calculating a similarity score based on the embeddings. The aim is to find the chunk that best matches the intent of the user's question.</p> </li> <li> <p>Answer Generation: Once the most similar chunk is identified, the AI model generates an answer based on the content associated with that chunk. This can be achieved through a chatbot-like mechanism or by employing predefined responses associated with the matched chunk.</p> </li> </ol> <p>If there is multiple files, the decomposition structure can be very different. Each filetype and specific file should be decomposed and handled in a distinct way, and then will be merged to a singular text embedding. If the number of files or number of rows of text data becomes large, it is recommended to store the embeddings of text in a vector database such as pinecone or weaviate for cloud based offerings, or chroma for local on prem solutions, that way the queries can run efficiently on large corpuses of data at scale.</p> <p></p> <p>Once the language model is loaded and trained, you can engage in a conversation with documents using Anote. This interactive capability allows you to interact with text documents and obtain specific information through a question-and-answer format. The model can process various types of documents, including financial documents, legal contracts, research papers, and more.</p> <p>To illustrate the functionality of Q&amp;A prompting, consider the following example. The picture showcases an example of using Anote to analyze a financial document consisting of 150 pages.</p> <p> </p> <p>The image presents a snapshot of the system's response to a specific query related to the financial document. The generated answer provides valuable insights and information extracted from the document, demonstrating the AI-powered capability of Anote in extracting relevant details from large volumes of text.</p> <p>One thing to note is that the LLM contains information and context outside of your document as well, pretrained on entire corpus of data to offer diverse perspectives. While the question and answering interface is tailored / fine tuned to your data, you can also ask questions that fall outside of the realm, as seen in the image below.</p> <p></p> <p>By leveraging Q&amp;A prompting, you can navigate through lengthy documents efficiently, pose specific questions, and receive targeted responses that assist in understanding and analyzing the document's content. This interactive and conversational approach enhances the effectiveness and productivity of document analysis tasks.</p>"},{"location":"prompting/questionanswering.html#improving-the-large-language-model-with-human-feedback","title":"Improving the Large Language Model with Human Feedback","text":"<p>While the Q&amp;A prompting approach aims to provide precise answers, it is important to acknowledge that there may be instances where the generated response is incorrect or lacks accuracy. This is where Anote truly excels. With the power of Anote, you have the ability to improve the overall accuracy by seamlessly integrating human feedback into the process. If the generated answer is identified as incorrect, you are presented with an opportunity to modify it based on your own knowledge and expertise. This iterative feedback loop not only refines the model's responses but also drives continuous improvement, ultimately boosting its performance and ensuring a higher level of accuracy.</p> <p>At Anote, we employ several techniques to enhance the accuracy of the large language model using human feedback. Once you prompt the model, it generates an answer based on the steps mentioned above. However, if the generated answer is incorrect or inaccurate, you have the option to provide your own answer in the dedicated feedback window.</p> <p>As you continue to correct the model's answers over time, the model learns from this feedback and adjusts its responses accordingly. Through incorporating human feedback, we continuously refine and enhance the large language model, ensuring that it becomes more accurate, reliable, and aligned with human knowledge and expertise. The goal is to reach a point where the model's predicted answer aligns with the human feedback you provide.</p>"},{"location":"prompting/questionanswering.html#user-feedback-loop","title":"User Feedback Loop","text":"<p>User feedback plays a significant role in improving the question answering capabilities of language models. By analyzing the user's feedback and comparing it with the generated answers, we can enhance the model's performance and accuracy over time.</p> <ul> <li> <p>Comparison with User Answers: By comparing the generated answer with the user's answer or expected answer, we can identify discrepancies and areas for improvement.</p> </li> <li> <p>Feedback Integration: Incorporating user feedback into the training process helps the model learn from its mistakes and refine its understanding of questions and answers.</p> </li> <li> <p>Adjustment of Prompts: Based on user feedback, we can adjust the prompts or questions given to the language model to guide it towards more accurate and relevant answers.</p> </li> <li> <p>Fine-tuning and Retraining: By including user-generated question-answer pairs in the training data, we can fine-tune and retrain the model to improve its performance in question answering tasks.</p> </li> </ul>"},{"location":"prompting/questionanswering.html#download-result","title":"Download Result","text":"<p>Once the model incorporates human feedback and achieves a satisfactory level of accuracy, users have the opportunity to download the report of the Q&amp;A process in a convenient table format. This report provides a structured overview of the questions posed, the model's predictions, and the actual human feedback for each question.</p> <p>The table format organizes the information in a tabular manner, allowing for easy readability and analysis. The report typically includes the following columns:</p> <ul> <li>Document: Represents the document or text from which the model is extracting information and generating responses. It can be a PDF, text file, or any other source of information.</li> <li>Question: Refers to the user-inputted question or prompt. It is the specific query or inquiry that the user wants the model to answer based on the given document.</li> <li>Response: Represents the model's predicted answer or response to the question. It is generated by the language model based on its understanding of the document and the provided question.</li> </ul> Document Question Pred Financial Management and Real Options What is financial management? Financial management is the process of managing money and other financial resources in order to achieve an organization's goals. It involves planning, organizing, controlling, and monitoring the financial activities of an organization. Financial Management and Real Options Can you explain the significance of cautiousness in financial management? Cautiousness is an important concept in financial management because it involves making decisions that are both financially sound and ethical. Cautiousness requires financial managers to consider the long-term consequences of their decisions and to act in the best interests of their organization. Financial Management and Real Options How does cautiousness contribute to risk management? Cautiousness contributes to risk management by ensuring that financial managers make decisions that are well-informed and carefully considered."},{"location":"prompting/questionanswering.html#summary","title":"Summary","text":"<p>Q&amp;A prompting using AI offers an effective and interactive way to extract specific information from text documents. By following the steps of text chunking, embedding, similarity matching, and answer generation, users can obtain precise answers to their questions. The iterative process of incorporating human feedback ensures continuous improvement, allowing the model to provide more accurate and reliable responses.</p>"},{"location":"prompting/semistructured.html","title":"Semi Structured Prompting","text":"<p>Semi-structured prompting refers to a method of querying or instructing a language model where the input includes both structured and unstructured elements. The structured prompts are typically categorical, directing the AI towards specific classes or categories of responses. An example of a structured prompt could be asking the customer to rate their satisfaction on a numeric scale from 1 to 5 (structured, categorical output).</p> <p>The unstructured prompts, on the other hand, are typically freeform, allowing a broader scope for the AI's responses. An example of an unstructured prompt could be requesting the customer to provide a written explanation for their rating (unstructured, free-form text output).</p> <p>Semi strucutured prompting encapsulates a wide variety of prompt types, including entity citations, chained prompts, or taxonomy-based tagging or classification or information extraction from text.</p> <p>Here are examples for each type of prompt mentioned:</p> <p>Entity Citations - This could involve prompting the AI to pull out specific information from a given resource. For instance, \"Using the provided URL, list the names of all presidents from the 18th century, in format <code>url | president_name</code>\"</p> <p>Chained Prompts - This involves a sequence of prompts that build upon each other. For example, a first prompt might be \"Describe the key events of World War II\", followed by \"Who were the major players involved in these events?\" and \"What were the global implications of these events?\"</p> <p>Information Extraction - Here, the AI might be asked to identify and extract specific entities or information from a block of text. An example could be, \"Identify and list all the names of people mentioned in the following news article.\"</p> <p>Multi-label Tagging/Classification - In this case, the AI would be prompted to categorize the input data based on a predefined taxonomy. For example, \"Classify the following list of animals into the categories: <code>mammals, birds, reptiles, amphibians, fish</code>.\"</p> <p>This combination of structured and unstructured prompting makes semi-structured prompting a versatile tool for interacting with AI systems. Semi-Structured Prompting is very helpful in the financial domain, for fine tuning models to provide more accurate results in relation to earnings call transcripts.</p> <p></p> <p>As seen, semi-structured prompting with active learning can be used to answer many questions on financial documents such as earnings call transcripts. It can also be very helpful for other financial documents, such as Annual Reports, 10-Qs or 10-Ks. Below, we will go through an example of leveraging active learning to improve the answers from 10-K documents with human feedback.</p>"},{"location":"prompting/semistructured.html#example-information-extraction-for-10-ks","title":"Example: Information Extraction for 10-Ks","text":"<p>Extracting information from long documents, such as 10-K reports, can be a time-consuming and challenging task. One approach to simplify the process is by leveraging the power of language models like ChatGPT, Claude, BARD or other Large Language Models (LLMs) to generate summaries based on given prompts.</p>"},{"location":"prompting/semistructured.html#upload-data","title":"Upload Data","text":"<p>Start by uploading the case studies in the Upload Unstructured format, choose the NLP task of Prompting, and choose the document decomposition.</p> <p></p> <p></p> <p></p>"},{"location":"prompting/semistructured.html#customize-questions","title":"Customize Questions","text":"<p>Input the relevant questions for the case studies. For example, to extract information from a 10-K report, you could prompt the language model with the following questions:</p> <p>Prompt: Summarize this document.</p> <p>Prompt: Is there a risk narrative in this document.</p> <p></p> <p>Once you provide a specific prompt or question that instructs the model to summarize the given text, the language model then uses its understanding of language patterns and context to generate a concise and coherent summary.</p>"},{"location":"prompting/semistructured.html#annotate","title":"Annotate","text":"<p>Insert human feedback via inputing the relevant structured and unstructured answers for the questions in the 10-Ks.</p> <p></p> <p>Over time, the model can learn to modify the prompt to be more tailored, such as:</p> <p>Prompt: can you summarize the financial performance and key highlights of the company mentioned in this 10-K report?</p> <p>Prompt: Can you identify the specific section, page and citation where the risk narrative occurs, if it is in the 10-K.</p>"},{"location":"prompting/semistructured.html#download","title":"Download","text":"<p>We can download the results from the model in a CSV,</p> <p></p> <p>For more details on Anote's work with 10-K's see the 10Ks section of our documentation.</p>"},{"location":"prompting/semistructured.html#conclusion","title":"Conclusion","text":"<p>Semi-structured prompting combined with user feedback is a powerful approach for analyzing 10-Ks. Being able to instantaneously obtain critical information a really long document, like a 10-K, can be very useful. For one, we can extract the key information without needing to manually go through the entire document by hand, saving us a lot of time and headache. In addition, because the Large Language Models have been trained on vast amounts of text data, they are able to comprehend the context and extract relevant information to generate accurate summaries, sometimes even better than humans. This saves the human the pain of go through a lot of the jargon to identify the key insights.</p>"},{"location":"prompting/summarization.html","title":"Summarization","text":"<p>Summarizing long documents can be a time-consuming and challenging task. One approach to simplify the process is by leveraging the power of large language models to generate summaries based on given prompts.</p>"},{"location":"prompting/summarization.html#example-semi-structured-summarization-for-resumes","title":"Example: Semi-Structured Summarization for Resumes","text":"<p>Analyzing resumes is a nuanced task that demands understanding of various components like education, skills, experience, and achievements. Employing the capabilities of large language models (LLMs) like ChatGPT with summarization and semi-structured prompting can significantly boost the effectiveness and precision of resume analysis. Here's how semi-structured summarization works in the context of a resume analysis on Anote, where we ask questions on resumes, and use human input to refine the predictions of LLMs.</p> <p>For example of how this can be applicable, to summarize a resume, you could provide a prompt like: <code>Summarize this resume in 2 sentences.</code></p>"},{"location":"prompting/summarization.html#upload-data","title":"Upload Data","text":"<p>Start by uploading the case studies in the Upload Unstructured format, choose the NLP task of Prompting, and choose the document decomposition.</p> <p></p> <p></p> <p></p>"},{"location":"prompting/summarization.html#customize-questions","title":"Customize Questions","text":"<p>Input the relevant questions for the case studies. For example, to extract information from a resume, you could prompt the language model with the following questions:</p> <p></p> <p>If you would prefer a more detailed response, you can always refine your prompt in the customize tab to something like the following prompt:</p> <p>Prompt: Please generate a summary highlighting the candidate's key skills, experiences, and achievements.</p>"},{"location":"prompting/summarization.html#annotate","title":"Annotate","text":"<p>The LLM will generate a response, given the question you ask and data you upload. For instance, your LLM might generate the following summary as output.</p> <p></p> <p>Generated Summary: John Doe is a Software Engineer with expertise in Python, Java, and C++. He has experience in developing web applications using Django and has strong problem-solving and analytical skills. He holds a Bachelor of Science in Computer Science and has received recognition for his exceptional performance and dedication at ABC Tech. Additionally, he has published a research paper on machine learning in a reputable conference.</p> <p>However, the generated summary might be different than the actual summary that we would want, so we may want to update the summary by hand to the following output.</p> <p>Actual Summary: John Doe is an experienced Software Engineer proficient in Python, Java, and C++. He has developed web applications using Django, collaborated with cross-functional teams.</p> <p></p> <p>You can imagine that as the amount of documents becomes relatively large, on the order of millions of documents, updating the summaries for each document by hand could become incredibly tedious and time consuming. On Anote, we can learn from human feedback to predict better summaries with just a few human interventions.</p>"},{"location":"prompting/summarization.html#user-feedback-the-trick-to-finding-better-summaries","title":"User Feedback - The Trick to Finding Better Summaries","text":"<p>User feedback plays a crucial role in refining the summarization process. Here is a high level guide on how we incorporate human feedback to find the ideal summaries across millions of documents with just a few interventions</p> <p>1. Embedding Space Comparison: By representing the generated summary and the user's summary in the embedding space, we can measure the similarity or dissimilarity between them. This comparison helps us quantify the differences and identify areas where the generated summary can be improved.</p> <p>2. Reprompting and Retraining: Based on the differences observed in the embedding space, we can re-prompt the language model with more specific or targeted prompts to guide the summarization process towards the desired output. Additionally, we can incorporate the user's own summary as training data to fine-tune the model and improve its performance over time.</p> <p>3. Iterative Refinement: Through an iterative process of gathering user feedback, analyzing differences, re-prompting, and retraining, we can continually refine the summarization model. This feedback loop allows us to adapt the language model to specific document types, address domain-related nuances, and produce more accurate and relevant summaries based on human preferences.</p>"},{"location":"prompting/summarization.html#download-results","title":"Download Results","text":"<p>In the table below, you can see the model response, and the human feedback. Depending on if the output of the model is structured (categorical) or unstructured (freeform), we leverage unique approaches to learn from the actual human feedback to adjust the model predictions to make them more tailored to the end user.</p> <p></p>"},{"location":"prompting/summarization.html#impact-on-businesses","title":"Impact on Businesses","text":"<p>Companies have an abundance of resumes on file but lack the time to thoroughly read each one. With Anote, recruiters, who used to manually reviewed 1,000 candidate resumes that matched the keywords \"Data Analyst\" and \"Python\", can obtain the critical information they need in a fraction of the time.</p>"},{"location":"prompting/summarization.html#conclusion","title":"Conclusion","text":"<p>Summarizing long documents using language models provides an efficient and automated approach. By leveraging user feedback and adjusting the summarization process based on differences observed in the embedding space, we can continuously improve the quality and accuracy of the generated summaries. After a few examples of human feedback, the summaries are able to become domain specific, which can save people a lot of time going through vast quantities of documents and data.</p>"}]}